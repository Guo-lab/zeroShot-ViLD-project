{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/fc/n5827zt91vb65jnqcsgpv5cr0000gn/T/ipykernel_16795/2159206710.py:254: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-16 17:43:10.322874: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_path_v2/variables/variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-16 17:43:12.636818: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "#@title Import libraries\n",
    "from easydict import EasyDict\n",
    "import collections\n",
    "import json\n",
    "import yaml\n",
    "import os\n",
    "import os.path as osp\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "import tensorflow.compat.v1 as tf # version .1\n",
    "from tqdm import tqdm\n",
    "from scipy.special import softmax\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import PIL.ImageColor as ImageColor\n",
    "import PIL.ImageDraw as ImageDraw\n",
    "import PIL.ImageFont as ImageFont\n",
    "\n",
    "\n",
    "#@title Define hyperparameters\n",
    "FLAGS = {\n",
    "    'prompt_engineering': True,\n",
    "    'this_is': True,\n",
    "    \n",
    "    'temperature': 100.0,\n",
    "    'use_softmax': False,\n",
    "}\n",
    "FLAGS = EasyDict(FLAGS)\n",
    "\n",
    "# Global matplotlib settings\n",
    "SMALL_SIZE = 16#10\n",
    "MEDIUM_SIZE = 18#12\n",
    "BIGGER_SIZE = 20#14\n",
    "plt.rc('font', size=MEDIUM_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "# Parameters for drawing figure.\n",
    "display_input_size = (10, 10)\n",
    "overall_fig_size = (18, 24)\n",
    "line_thickness = 2\n",
    "fig_size_w = 35\n",
    "# fig_size_h = min(max(5, int(len(category_names) / 2.5) ), 10)\n",
    "mask_color =   'red'\n",
    "alpha = 0.5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def article(name):  #% A/An\n",
    "  return 'an' if name[0] in 'aeiou' else 'a'\n",
    "\n",
    "def processed_name(name, rm_dot=False):  #% '_' for lvis, '/' for obj365\n",
    "  res = name.replace('_', ' ').replace('/', ' or ').lower()\n",
    "  if rm_dot:\n",
    "    res = res.rstrip('.')\n",
    "  return res\n",
    "\n",
    "\n",
    "# //////////////////////////////////////////////////////////////\n",
    "single_template = [  #% Only one\n",
    "    'a photo of {article} {}.'\n",
    "]\n",
    "# //////////////////////////////////////////////////////////////\n",
    "multiple_templates = [\n",
    "    'There is {article} {} in the scene.',\n",
    "    'There is the {} in the scene.',\n",
    "    'a photo of {article} {} in the scene.',\n",
    "    'a photo of the {} in the scene.',\n",
    "    'a photo of one {} in the scene.',\n",
    "\n",
    "    'itap of {article} {}.',\n",
    "    'itap of my {}.',  # itap: I took a picture of\n",
    "    'itap of the {}.',\n",
    "    \n",
    "    'a photo of {article} {}.',\n",
    "    'a photo of my {}.',\n",
    "    'a photo of the {}.',\n",
    "    'a photo of one {}.',\n",
    "    'a photo of many {}.',\n",
    "\n",
    "    'a good photo of {article} {}.',\n",
    "    'a good photo of the {}.',\n",
    "    'a bad photo of {article} {}.',\n",
    "    'a bad photo of the {}.',\n",
    "    'a photo of a nice {}.',\n",
    "    'a photo of the nice {}.',\n",
    "    'a photo of a cool {}.',\n",
    "    'a photo of the cool {}.',\n",
    "    'a photo of a weird {}.',\n",
    "    'a photo of the weird {}.',\n",
    "\n",
    "    'a photo of a small {}.',\n",
    "    'a photo of the small {}.',\n",
    "    'a photo of a large {}.',\n",
    "    'a photo of the large {}.',\n",
    "\n",
    "    'a photo of a clean {}.',\n",
    "    'a photo of the clean {}.',\n",
    "    'a photo of a dirty {}.',\n",
    "    'a photo of the dirty {}.',\n",
    "\n",
    "    'a bright photo of {article} {}.',\n",
    "    'a bright photo of the {}.',\n",
    "    'a dark photo of {article} {}.',\n",
    "    'a dark photo of the {}.',\n",
    "\n",
    "    'a photo of a hard to see {}.',\n",
    "    'a photo of the hard to see {}.',\n",
    "    'a low resolution photo of {article} {}.',\n",
    "    'a low resolution photo of the {}.',\n",
    "    'a cropped photo of {article} {}.',\n",
    "    'a cropped photo of the {}.',\n",
    "    'a close-up photo of {article} {}.',\n",
    "    'a close-up photo of the {}.',\n",
    "    'a jpeg corrupted photo of {article} {}.',\n",
    "    'a jpeg corrupted photo of the {}.',\n",
    "    'a blurry photo of {article} {}.',\n",
    "    'a blurry photo of the {}.',\n",
    "    'a pixelated photo of {article} {}.',\n",
    "    'a pixelated photo of the {}.',\n",
    "\n",
    "    'a black and white photo of the {}.',\n",
    "    'a black and white photo of {article} {}.',\n",
    "\n",
    "    'a plastic {}.',\n",
    "    'the plastic {}.',\n",
    "\n",
    "    'a toy {}.',\n",
    "    'the toy {}.',\n",
    "    'a plushie {}.',\n",
    "    'the plushie {}.',\n",
    "    'a cartoon {}.',\n",
    "    'the cartoon {}.',\n",
    "\n",
    "    'an embroidered {}.',\n",
    "    'the embroidered {}.',\n",
    "\n",
    "    'a painting of the {}.',\n",
    "    'a painting of a {}.',\n",
    "]\n",
    "# //////////////////////////////////////////////////////////////\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clip.available_models()\n",
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#///////////////////////////////////////////\n",
    "#     Build text embedding function:\n",
    "def build_text_embedding(categories):\n",
    "  if FLAGS.prompt_engineering:  ## prompt engineering => some templates\n",
    "    templates = multiple_templates\n",
    "  else:\n",
    "    templates = single_template\n",
    "\n",
    "  with torch.no_grad():\n",
    "    all_text_embeddings = []  ## All Text Embeddings\n",
    "    print('Building text embeddings...')\n",
    "    \n",
    "    for category in tqdm(categories):\n",
    "      texts = [\n",
    "        template.format(processed_name(category['name'], rm_dot=True),\n",
    "                        article=article(category['name']))\n",
    "        for template in templates\n",
    "      ]\n",
    "      if FLAGS.this_is:\n",
    "        texts = [\n",
    "                 'This is ' + text if text.startswith('a') or text.startswith('the') else text \n",
    "                 for text in texts\n",
    "                 ]\n",
    "      texts = clip.tokenize(texts) #@ Returns a LongTensor containing tokenized sequences of given text input(s).\n",
    "      #TODO print(\"texts[tokenized sequences of given text inputs]: \", texts) ## This can be used as the input to the model.\n",
    "      if torch.cuda.is_available():\n",
    "        texts = texts.cuda()\n",
    "      \n",
    "      #@ Given a batch of text tokens, returns the text features encoded by the language portion of the CLIP model.  \n",
    "      text_embeddings = model.encode_text(texts) #embed with text encoder \n",
    "      text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "      text_embedding = text_embeddings.mean(dim=0)\n",
    "      text_embedding /= text_embedding.norm()\n",
    "      all_text_embeddings.append(text_embedding)\n",
    "    ## FOR ENDING  \n",
    "    all_text_embeddings = torch.stack(all_text_embeddings, dim=1)\n",
    "    if torch.cuda.is_available():\n",
    "      all_text_embeddings = all_text_embeddings.cuda()\n",
    "  ## WITH ENDING  \n",
    "  return all_text_embeddings.cpu().numpy().T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Helper Function ##\n",
    "numbered_categories = [{'name': str(idx), 'id': idx,} for idx in range(50)]\n",
    "# print(numbered_categories)  # [{'name': '0', 'id': 0}, {, ..., {'name': '49', 'id': 49}]\n",
    "numbered_category_indices = {cat['id']: cat for cat in numbered_categories}\n",
    "# print(numbered_category_indices)  # 0: {'name': '0', 'id': 0}, ..., 49: {'name': '49', 'id': 49}}\n",
    "\n",
    "#@ Non Maximum Suppression\n",
    "#@ dets: [N, 4] \n",
    "#@ scores: [N,]\n",
    "#@ thresh[Float]: iou threshold.\n",
    "#@ max_dets[Int]\n",
    "def nms(dets, scores, thresh, max_dets=1000):\n",
    "  y1 = dets[:, 0]\n",
    "  x1 = dets[:, 1]\n",
    "  y2 = dets[:, 2]\n",
    "  x2 = dets[:, 3]\n",
    "\n",
    "  areas = (x2 - x1) * (y2 - y1)\n",
    "  order = scores.argsort()[::-1]\n",
    "\n",
    "  keep = []\n",
    "  while order.size > 0 and len(keep) < max_dets:\n",
    "    i = order[0]\n",
    "    keep.append(i)\n",
    "\n",
    "    xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "    yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "    xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "    yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "    w = np.maximum(0.0, xx2 - xx1)\n",
    "    h = np.maximum(0.0, yy2 - yy1)\n",
    "    intersection = w * h\n",
    "    overlap = intersection / (areas[i] + areas[order[1:]] - intersection + 1e-12)\n",
    "\n",
    "    inds = np.where(overlap <= thresh)[0]\n",
    "    order = order[inds + 1]\n",
    "  ## WHILE ENDING \n",
    "  return keep\n",
    "\n",
    "\n",
    "## Graph and Session Init ##\n",
    "session = tf.Session(graph=tf.Graph())\n",
    "saved_model_dir = './image_path_v2' #@param {type:\"string\"}\n",
    "_ = tf.saved_model.loader.load(session, ['serve'], saved_model_dir) # Restoring parameters from ./image_path_v2/variables/variables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def paste_instance_masks(masks, detected_boxes, image_height, image_width):\n",
    "  def expand_boxes(boxes, scale):\n",
    "    \"\"\"Expands an array of boxes by a given scale.\"\"\"\n",
    "    w_half = boxes[:, 2] * .5\n",
    "    h_half = boxes[:, 3] * .5\n",
    "    x_c = boxes[:, 0] + w_half\n",
    "    y_c = boxes[:, 1] + h_half\n",
    "\n",
    "    w_half *= scale\n",
    "    h_half *= scale\n",
    "\n",
    "    boxes_exp = np.zeros(boxes.shape)\n",
    "    boxes_exp[:, 0] = x_c - w_half\n",
    "    boxes_exp[:, 2] = x_c + w_half\n",
    "    boxes_exp[:, 1] = y_c - h_half\n",
    "    boxes_exp[:, 3] = y_c + h_half\n",
    "\n",
    "    return boxes_exp\n",
    "  \n",
    "  _, mask_height, mask_width = masks.shape\n",
    "  scale = max((mask_width + 2.0) / mask_width, (mask_height + 2.0) / mask_height)\n",
    "\n",
    "  ref_boxes = expand_boxes(detected_boxes, scale)  ## Expand the boxes\n",
    "  ref_boxes = ref_boxes.astype(np.int32)\n",
    "  padded_mask = np.zeros((mask_height + 2, mask_width + 2), dtype=np.float32)\n",
    "  segms = []\n",
    "  for mask_ind, mask in enumerate(masks):\n",
    "    im_mask = np.zeros((image_height, image_width), dtype=np.uint8)\n",
    "    padded_mask[1:-1, 1:-1] = mask[:, :]  # Process mask inside bounding boxes.\n",
    "\n",
    "    ref_box = ref_boxes[mask_ind, :]\n",
    "    w = ref_box[2] - ref_box[0] + 1\n",
    "    h = ref_box[3] - ref_box[1] + 1\n",
    "    w = np.maximum(w, 1)\n",
    "    h = np.maximum(h, 1)\n",
    "\n",
    "    mask = cv2.resize(padded_mask, (w, h))\n",
    "    mask = np.array(mask > 0.5, dtype=np.uint8)\n",
    "\n",
    "    x_0 = min(max(ref_box[0], 0), image_width)\n",
    "    x_1 = min(max(ref_box[2] + 1, 0), image_width)\n",
    "    y_0 = min(max(ref_box[1], 0), image_height)\n",
    "    y_1 = min(max(ref_box[3] + 1, 0), image_height)\n",
    "\n",
    "    im_mask[y_0:y_1, x_0:x_1] = mask[\n",
    "        (y_0 - ref_box[1]):(y_1 - ref_box[1]),\n",
    "        (x_0 - ref_box[0]):(x_1 - ref_box[0])\n",
    "    ]\n",
    "    segms.append(im_mask)\n",
    "\n",
    "  segms = np.array(segms)\n",
    "  assert masks.shape[0] == segms.shape[0]\n",
    "  \n",
    "  return segms   #@ Return numpy array [N, image_height, image_width] => the instance masks pasted on the image canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Visualization\n",
    "STANDARD_COLORS = ['AliceBlue', 'Chartreuse', 'Aqua', 'BlueViolet', 'DarkOrange', 'DeepPink', 'DeepSkyBlue', 'GhostWhite', 'Gold', 'LightBlue', 'LightCoral', 'LightSalmon', 'LightSeaGreen', 'LightSkyBlue', 'LightSlateGray', 'LightSlateGrey', 'LightSteelBlue', 'Olive', 'Orange', 'Purple', 'RosyBrown', 'RoyalBlue', 'SaddleBrown', 'Green', 'SandyBrown', 'SeaGreen', 'SeaShell', 'Sienna', 'Silver','Turquoise', 'Violet', 'Wheat', 'White', 'WhiteSmoke', 'Yellow', 'YellowGreen']\n",
    "  \n",
    "#* #########################################################\n",
    "#@ Draw_boxes function\n",
    "#@ image: a PIL.Image object.\n",
    "#@ display_str_list: list of strings to display in box (out of range => displaying below the box)\n",
    "#@ use_normalized_coordinates: True (default) => treat coordinates as relative to the image. Otherwise => absolute.\n",
    "def draw_bounding_box_on_image(image, ymin, xmin, ymax, xmax, \n",
    "                               color='red', thickness=4,\n",
    "                               display_str_list=(),\n",
    "                               use_normalized_coordinates=True,user=0,):\n",
    "  if user==2:\n",
    "    return\n",
    "  \n",
    "  draw = ImageDraw.Draw(image)\n",
    "  im_width, im_height = image.size\n",
    "  if use_normalized_coordinates:\n",
    "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width, ymin * im_height, ymax * im_height)\n",
    "  else:\n",
    "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
    "  draw.line([(left, top), (left, bottom), (right, bottom), (right, top), (left, top)], width=thickness, fill=color)\n",
    "  \n",
    "  try:\n",
    "    font = ImageFont.truetype('arial.ttf', 24)\n",
    "  except IOError:\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "  #@ Displayed string position adjustment \n",
    "  display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n",
    "  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)   # display_str has top and bottom margin_(0.05x).\n",
    "  if top > total_display_str_height:\n",
    "    text_bottom = top\n",
    "  else:\n",
    "    text_bottom = bottom + total_display_str_height\n",
    "    \n",
    "  for display_str in display_str_list[::-1]:  \n",
    "    text_left = min(5, left)\n",
    "    text_width, text_height = font.getsize(display_str)\n",
    "    margin = np.ceil(0.05 * text_height)\n",
    "    draw.rectangle([(left, text_bottom - text_height - 2 * margin), (left + text_width, text_bottom)], fill=color)\n",
    "    draw.rectangle([(left + text_width, text_bottom - text_height - 2 * margin), (right + margin, text_bottom)], fill='LightGreen')\n",
    "    \n",
    "    draw.text((left + margin, text_bottom - text_height - margin),display_str, fill='black', font=font)\n",
    "    #draw.text((left + margin + text_width*2, text_bottom - text_height - margin), display_str, fill='black', font=font)\n",
    "    text_bottom -= text_height - 2 * margin\n",
    "\n",
    "\n",
    "\n",
    "#* #########################################################\n",
    "#@ Draw_box on image (numpy array form) {CALL draw_bounding_box_on_image()}\n",
    "#@ image: a numpy array with shape [height, width, 3].\n",
    "def draw_bounding_box_on_image_array(image, ymin, xmin, ymax, xmax, user=0,\n",
    "                                     color='red', thickness=4,\n",
    "                                     display_str_list=(),\n",
    "                                     use_normalized_coordinates=True):\n",
    "  image_pil = Image.fromarray(np.uint8(image)).convert('RGB')\n",
    "  #print(user)\n",
    "  draw_bounding_box_on_image(image_pil, ymin, xmin, ymax, xmax, color, thickness, display_str_list, use_normalized_coordinates, user=user)\n",
    "  np.copyto(image, np.array(image_pil))  # image flush\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def draw_mask_on_image_array(image, mask, color='red', alpha=0.4):\n",
    "  if image.dtype != np.uint8:\n",
    "    raise ValueError('`image` not of type np.uint8')\n",
    "  if mask.dtype != np.uint8:\n",
    "    raise ValueError('`mask` not of type np.uint8')\n",
    "  if np.any(np.logical_and(mask != 1, mask != 0)):\n",
    "    raise ValueError('`mask` elements should be in [0, 1]')\n",
    "  if image.shape[:2] != mask.shape:\n",
    "    raise ValueError('The image has spatial dimensions %s, but the mask has dimensions %s' % (image.shape[:2], mask.shape))\n",
    "  rgb = ImageColor.getrgb(color)\n",
    "  pil_image = Image.fromarray(image)\n",
    "  solid_color = np.expand_dims(np.ones_like(mask), axis=2) * np.reshape(list(rgb), [1, 1, 3])\n",
    "  pil_solid_color = Image.fromarray(np.uint8(solid_color)).convert('RGBA')\n",
    "  pil_mask = Image.fromarray(np.uint8(255.0*alpha*mask)).convert('L')\n",
    "  pil_image = Image.composite(pil_solid_color, pil_image, pil_mask)\n",
    "  np.copyto(image, np.array(pil_image.convert('RGB')))\n",
    "  \n",
    "def plot_mask(color, alpha, original_image, mask):  #! Simialr to draw_mask_on_image_array()\n",
    "  rgb = ImageColor.getrgb(color)\n",
    "  pil_image = Image.fromarray(original_image)\n",
    "\n",
    "  solid_color = np.expand_dims(np.ones_like(mask), axis=2) * np.reshape(list(rgb), [1, 1, 3])\n",
    "  pil_solid_color = Image.fromarray(np.uint8(solid_color)).convert('RGBA')\n",
    "  pil_mask = Image.fromarray(np.uint8(255.0*alpha*mask)).convert('L')\n",
    "  pil_image = Image.composite(pil_solid_color, pil_image, pil_mask)\n",
    "  ## Maintain the original image\n",
    "  img_w_mask = np.array(pil_image.convert('RGB'))\n",
    "  return img_w_mask\n",
    "\n",
    "\n",
    "\n",
    "#* #########################################################\n",
    "#@ Visualize labeled boxes, scores\n",
    "#@ image: uint8 numpy array [img_height, img_width, 3]\n",
    "#@ boxes: numpy array [N, 4]\n",
    "#@ classes: numpy array [N]. Note that class indices are 1-based and match the keys in the label map.\n",
    "#@ scores: numpy array [N] or None(assumes plotted boxes are groundtruth boxes. Plot all boxes as black without classes/scores)\n",
    "#@ category_index: a dict [[category index `id`, category name `name`] keyed by category indices]\n",
    "def visualize_boxes_and_labels_on_image_array(image, boxes, classes, scores, category_index,user,\n",
    "                                              instance_masks=None, instance_boundaries=None,\n",
    "                                              use_normalized_coordinates=False,\n",
    "                                              max_boxes_to_draw=20, min_score_thresh=.5, #max number of boxes and  minimum score to visualize\n",
    "                                              agnostic_mode=False,\n",
    "                                              line_thickness=4,\n",
    "                                              groundtruth_box_visualization_color='black',\n",
    "                                              skip_scores=False, skip_labels=False,\n",
    "                                              mask_alpha=0.4,\n",
    "                                              plot_color=None,):\n",
    "  box_to_display_str_map = collections.defaultdict(list)\n",
    "  box_to_color_map = collections.defaultdict(str)\n",
    "  box_to_instance_masks_map = {}\n",
    "  box_to_score_map = {}\n",
    "  box_to_instance_boundaries_map = {}\n",
    "  \n",
    "  #print(user)\n",
    "  \n",
    "  if not max_boxes_to_draw:\n",
    "    max_boxes_to_draw = boxes.shape[0]\n",
    "\n",
    "  #//print(\"boxes.shape: \", boxes.shape[0])\n",
    "  for i in range(min(max_boxes_to_draw, boxes.shape[0])):\n",
    "    if scores is None or scores[i] > min_score_thresh:\n",
    "      box = tuple(boxes[i].tolist())\n",
    "      if instance_masks is not None:\n",
    "        box_to_instance_masks_map[box] = instance_masks[i]\n",
    "      if instance_boundaries is not None:\n",
    "        box_to_instance_boundaries_map[box] = instance_boundaries[i]\n",
    "      if scores is None:\n",
    "        box_to_color_map[box] = groundtruth_box_visualization_color\n",
    "      \n",
    "      else:\n",
    "        display_str = ''\n",
    "        if not skip_labels:\n",
    "          if not agnostic_mode:\n",
    "            #print(list(category_index.keys()))\n",
    "            if classes[i] in list(category_index.keys()):\n",
    "              #print(classes[i])\n",
    "              class_name = category_index[classes[i]]['name']\n",
    "              #print(class_name)\n",
    "            else:\n",
    "              class_name = 'N/A'\n",
    "            display_str = str(class_name)\n",
    "            #print(display_str)\n",
    "            \n",
    "        ## SKIP_LABELS IF ENDING\n",
    "        if not skip_scores:\n",
    "          if not display_str:\n",
    "            display_str = '{}%'.format(int(100*scores[i]))\n",
    "          else:\n",
    "            float_score = (\"%.2f\" % scores[i]).lstrip('0')\n",
    "            display_str = '{}: {}'.format(display_str, float_score)\n",
    "          box_to_score_map[box] = int(100*scores[i])\n",
    "          \n",
    "        ## SKIP_SCORES IF ENDING \n",
    "        box_to_display_str_map[box].append(display_str)\n",
    "        if plot_color is not None:\n",
    "          box_to_color_map[box] = plot_color\n",
    "        elif agnostic_mode:\n",
    "          box_to_color_map[box] = 'DarkOrange'\n",
    "        else:\n",
    "          box_to_color_map[box] = STANDARD_COLORS[\n",
    "              classes[i] % len(STANDARD_COLORS)]\n",
    "  \n",
    "  ## Handle the case when box_to_score_map is empty.\n",
    "  if box_to_score_map:\n",
    "    box_color_iter = sorted(box_to_color_map.items(), key=lambda kv: box_to_score_map[kv[0]])\n",
    "  else:\n",
    "    box_color_iter = box_to_color_map.items()\n",
    "\n",
    "  ## Draw all boxes onto image.\n",
    "  ## UI Change parameter: user ##\n",
    "  for box_color, tmpUser in zip(box_color_iter, user):\n",
    "    box, color = box_color\n",
    "    ymin, xmin, ymax, xmax = box\n",
    "    if instance_masks is not None:\n",
    "      draw_mask_on_image_array(image, box_to_instance_masks_map[box], color=color, alpha=mask_alpha)\n",
    "    if instance_boundaries is not None:\n",
    "      draw_mask_on_image_array(image, box_to_instance_boundaries_map[box], color='red', alpha=1.0)\n",
    "      \n",
    "    draw_bounding_box_on_image_array(image, ymin, xmin, ymax, xmax, user=tmpUser,\n",
    "                                     color=color, thickness=line_thickness,\n",
    "                                     display_str_list=box_to_display_str_map[box],\n",
    "                                     use_normalized_coordinates=use_normalized_coordinates)\n",
    "    \n",
    "    #print(type(image))\n",
    "    im = Image.fromarray(image)\n",
    "    im.save(\"tmp/ui.jpg\")\n",
    "    #//cv2.imwrite(\"tmp/ui.png\", image)\n",
    "  return image   #@ Return uint8 numpy array [img_height, img_width, 3] with overlaid boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#\n",
    "# Main functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fc/n5827zt91vb65jnqcsgpv5cr0000gn/T/ipykernel_16795/2881931030.py:52: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.isin(np.arange(len(roi_scores), dtype=np.int), nmsed_indices),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid indices 254\n",
      "Building text embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.73s/it]\n"
     ]
    }
   ],
   "source": [
    "image_path = './examples/test.jpg'  #@param {type:\"string\"}\n",
    "max_boxes_to_draw = 5 #@param {type:\"integer\"}\n",
    "nms_threshold = 0.6 #@param {type:\"slider\", min:0, max:0.9, step:0.05} -- Non Maximum Suppression\n",
    "min_rpn_score_thresh = 0.9  #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "min_box_area = 220 #@param {type:\"slider\", min:0, max:10000, step:1.0}\n",
    "category_name_string = ';'.join([\"girl\"])\n",
    "# image_path, \n",
    "# category_name_string, \n",
    "#numbered_categories = [{'name': str(idx), 'id': idx,} for idx in range(50)]\n",
    "# print(numbered_categories)  # [{'name': '0', 'id': 0}, {, ..., {'name': '49', 'id': 49}]\n",
    "#numbered_category_indices = {cat['id']: cat for cat in numbered_categories}\n",
    "# print(numbered_category_indices)  # 0: {'name': '0', 'id': 0}, ..., 49: {'name': '49', 'id': 49}}\n",
    "category_names = [x.strip() for x in category_name_string.split(';')]\n",
    "category_names = ['background'] + category_names\n",
    "categories = [{'name': item, 'id': idx+1,} for idx, item in enumerate(category_names)]\n",
    "category_indices = {cat['id']: cat for cat in categories}  ## The case in Helper Function\n",
    "fig_size_h = min(max(5, int(len(category_names) / 2.5) ), 10)\n",
    "\n",
    "roi_boxes, roi_scores, detection_boxes, scores_unused, box_outputs, detection_masks, visual_features, image_info \\\n",
    "= session.run(\n",
    "    ['RoiBoxes:0', 'RoiScores:0', '2ndStageBoxes:0', '2ndStageScoresUnused:0', 'BoxOutputs:0', 'MaskOutputs:0', 'VisualFeatOutputs:0', 'ImageInfo:0'], # fetches\n",
    "    feed_dict={'Placeholder:0': [image_path, ] }\n",
    "  )\n",
    "\n",
    "roi_boxes = np.squeeze(roi_boxes, axis=0)  # squeeze\n",
    "roi_scores = np.squeeze(roi_scores, axis=0)\n",
    "detection_boxes = np.squeeze(detection_boxes, axis=(0, 2))\n",
    "scores_unused = np.squeeze(scores_unused, axis=0)\n",
    "box_outputs = np.squeeze(box_outputs, axis=0)\n",
    "detection_masks = np.squeeze(detection_masks, axis=0)\n",
    "visual_features = np.squeeze(visual_features, axis=0)\n",
    "image_info = np.squeeze(image_info, axis=0)  # obtain image info\n",
    "\n",
    "image_scale = np.tile(image_info[2:3, :], (1, 2))\n",
    "image_height = int(image_info[0, 0])\n",
    "image_width = int(image_info[0, 1])\n",
    "rescaled_detection_boxes = detection_boxes / image_scale # rescale\n",
    "\n",
    "## Read image\n",
    "image = np.asarray(Image.open(open(image_path, 'rb')).convert(\"RGB\"))\n",
    "assert image_height == image.shape[0]\n",
    "assert image_width == image.shape[1]\n",
    "\n",
    "\n",
    "## Apply non-maximum suppression to detected boxes with nms threshold.\n",
    "nmsed_indices = nms(detection_boxes, roi_scores, thresh=nms_threshold)\n",
    "## Compute RPN box size.\n",
    "box_sizes = (rescaled_detection_boxes[:, 2] - rescaled_detection_boxes[:, 0]) * (rescaled_detection_boxes[:, 3] - rescaled_detection_boxes[:, 1])\n",
    "## Filter out invalid rois (nms-okay rois)\n",
    "valid_indices = np.where(\n",
    "    np.logical_and(\n",
    "                  np.isin(np.arange(len(roi_scores), dtype=np.int), nmsed_indices),\n",
    "                  np.logical_and(\n",
    "                                np.logical_not(np.all(roi_boxes == 0., axis=-1)),\n",
    "                                np.logical_and(\n",
    "                                              roi_scores >= min_rpn_score_thresh,\n",
    "                                              box_sizes > min_box_area \n",
    "                                              ))))[0]\n",
    "print('number of valid indices', len(valid_indices))\n",
    "\n",
    "detection_roi_scores = roi_scores[valid_indices][:max_boxes_to_draw, ...]\n",
    "detection_boxes = detection_boxes[valid_indices][:max_boxes_to_draw, ...]\n",
    "detection_masks = detection_masks[valid_indices][:max_boxes_to_draw, ...]\n",
    "#### UI ####\n",
    "isSaved = np.zeros(detection_roi_scores.shape)\n",
    "detection_visual_feat = visual_features[valid_indices][:max_boxes_to_draw, ...]  ## Get Detection Visual Feat\n",
    "rescaled_detection_boxes = rescaled_detection_boxes[valid_indices][:max_boxes_to_draw, ...]  ## Get Rescaled Detection Boxes \n",
    "\n",
    "text_features = build_text_embedding(categories)\n",
    "    \n",
    "raw_scores = detection_visual_feat.dot(text_features.T) ## From visual features\n",
    "if FLAGS.use_softmax:\n",
    "  scores_all = softmax(FLAGS.temperature * raw_scores, axis=-1)\n",
    "else:\n",
    "  scores_all = raw_scores\n",
    "\n",
    "indices = np.argsort(-np.max(scores_all, axis=1))  # Results are ranked by scores\n",
    "indices_fg = np.array([i for i in indices if np.argmax(scores_all[i]) != 0])\n",
    "\n",
    "ymin, xmin, ymax, xmax = np.split(rescaled_detection_boxes, 4, axis=-1)\n",
    "processed_boxes = np.concatenate([xmin, ymin, xmax - xmin, ymax - ymin], axis=-1)\n",
    "segmentations = paste_instance_masks(detection_masks, processed_boxes, image_height, image_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Region Pos Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot detected boxes on the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui qt\n",
    "from PyQt5 import QtCore, QtGui, QtWidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START UI\n",
      "Have Button 2\n"
     ]
    }
   ],
   "source": [
    "from PyQt5.QtCore import *\n",
    "from PyQt5.QtGui import *\n",
    "from PyQt5.QtWidgets import *\n",
    "\n",
    "print(\"START UI\")\n",
    "\n",
    "\n",
    "'''\n",
    "########## First Window ###########\n",
    "###################################\n",
    "class firstWindow(QMainWindow):\n",
    "  def __init__(self, parent=None):\n",
    "    super(resultLabel, self).__init__((parent))\n",
    "'''\n",
    "\n",
    "########## Result Label ###########\n",
    "###################################\n",
    "class MyLabel(QLabel):\n",
    "  def __init__(self, parent=None):\n",
    "    super(resultLabel, self).__init__((parent))\n",
    "    self.x0 = 0\n",
    "    self.y0 = 0\n",
    "    self.x1 = 0\n",
    "    self.y1 = 0\n",
    "    self.flag = False\n",
    "    self.index = 0\n",
    "\n",
    "  def mousePressEvent(self,event):\n",
    "    self.flag = True\n",
    "    self.x0 = event.x()\n",
    "    self.y0 = event.y()\n",
    "    #//print(self.x0, self.y0)\n",
    "    self.index = 0\n",
    "    ## TODO Area Cverlap ## \n",
    "    for self.index in indices_fg:\n",
    "      #//print(self.index)\n",
    "      ymin, xmin, ymax, xmax = rescaled_detection_boxes[self.index]\n",
    "      #//print(ymin, xmin, ymax, xmax)\n",
    "      if self.x0 < xmax and self.x0 > xmin and self.y0 < ymax and self.y0 > ymin:\n",
    "        self.cb = QComboBox(self)\n",
    "        self.cb.move(self.x0, self.y0)\n",
    "        self.cb.addItem('This bbox is selected')\n",
    "        self.cb.addItem(QtGui.QIcon(\"./icon/icons8-assessments-90.png\"),'Get its results')\n",
    "        self.cb.addItem(QtGui.QIcon(\"./icon/icons8-multiplication-90.png\"),'Discard it')\n",
    "        self.cb.setCurrentIndex(0)\n",
    "        \n",
    "        self.cb.currentIndexChanged[str].connect(self.oper)\n",
    "        self.cb.show()\n",
    "        break\n",
    "        \n",
    "  def oper(self, choice):\n",
    "    #//print(i)\n",
    "    if choice == \"Get its results\":\n",
    "      #//print(self.index)\n",
    "      # Global Variable\n",
    "      isSaved[self.index] = 1\n",
    "      self.mgb = QMessageBox(self)\n",
    "      self.mgb.setIcon(QMessageBox.Information)\n",
    "      self.mgb.setText(\"You have saved the detection result.\")\n",
    "      self.mgb.show()\n",
    "      \n",
    "    elif choice == \"Discard it\":\n",
    "      #//print(self.index)\n",
    "      # Global Variable\n",
    "      isSaved[self.index] = 2\n",
    "      self.mgb = QMessageBox(self)\n",
    "      self.mgb.setIcon(QMessageBox.Warning)\n",
    "      self.mgb.setText(\"You would ignore this bbox!\")\n",
    "      self.mgb.setInformativeText(\"Later, it will not be saved\")\n",
    "      self.mgb.show()\n",
    "      \n",
    "    self.cb.deleteLater()\n",
    "    # Update Results\n",
    "    image_with_detections = \\\n",
    "    visualize_boxes_and_labels_on_image_array( \\\n",
    "      np.array(image),\n",
    "      rescaled_detection_boxes[indices_fg],\n",
    "      valid_indices[:max_boxes_to_draw][indices_fg],\n",
    "      detection_roi_scores[indices_fg],   \n",
    "      category_indices, #numbered_category_indices,\n",
    "      user=isSaved[indices_fg], \n",
    "      instance_masks=segmentations[indices_fg],\n",
    "      use_normalized_coordinates=False,\n",
    "      max_boxes_to_draw=max_boxes_to_draw, min_score_thresh=min_rpn_score_thresh,\n",
    "      skip_scores=False, skip_labels=False,\n",
    "    )\n",
    "    \n",
    "    img = cv2.imread(\"tmp/ui.jpg\")\n",
    "    height, width, bytesPerComponent = img.shape\n",
    "    self.setGeometry(QRect(0, 0, width, height))\n",
    "    bytesPerLine = 3 * width\n",
    "    cv2.cvtColor(img, cv2.COLOR_BGR2RGB, img)\n",
    "    QImg = QImage(img.data, width, height, bytesPerLine, QImage.Format_RGB888)\n",
    "    pixmap = QPixmap.fromImage(QImg)\n",
    "    self.setPixmap(pixmap)\n",
    "    self.setCursor(Qt.CrossCursor)\n",
    "    \n",
    "  def mouseReleaseEvent(self,event):\n",
    "    self.flag = False\n",
    "    \n",
    "  def mouseMoveEvent(self,event):\n",
    "    if self.flag:\n",
    "      self.x1 = event.x()\n",
    "      self.y1 = event.y()\n",
    "      self.update()\n",
    "  '''    \n",
    "  def paintEvent(self, event):\n",
    "    super().paintEvent(event)\n",
    "    rect = QRect(self.x0, self.y0, abs(self.x1-self.x0), abs(self.y1-self.y0))\n",
    "    painter = QPainter(self)\n",
    "    painter.setPen(QPen(Qt.red, 2, Qt.SolidLine))\n",
    "    #*painter.drawRect(rect)\n",
    "  '''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## UI ##\n",
    "########\n",
    "x_0 = QWidget()\n",
    "\n",
    "x_0.setWindowTitle('Detect objects. Please input custom labels.')\n",
    "\n",
    "\n",
    "x_0.lb_0 = QLabel(x_0) \n",
    "\n",
    "#* original image\n",
    "image_path = './examples/test.jpg'  #@param {type:\"string\"}\n",
    "img = cv2.imread(\"examples/test.jpg\")\n",
    "  \n",
    "height, width, bytesPerComponent = img.shape\n",
    "x_0.resize(width,height)\n",
    "x_0.lb_0.setGeometry(QRect(0, 0, width, height))\n",
    "x_0.move(int((QDesktopWidget().screenGeometry().width()-x_0.width())/2),int((QDesktopWidget().screenGeometry().height()-x_0.height())/2))\n",
    "#//print(QDesktopWidget().screenGeometry().width()-x_0.width(),QDesktopWidget().screenGeometry().height()-x_0.height())\n",
    "\n",
    "bytesPerLine = 3 * width\n",
    "\n",
    "cv2.cvtColor(img, cv2.COLOR_BGR2RGB, img)\n",
    "QImg = QImage(img.data, width, height, bytesPerLine, QImage.Format_RGB888)\n",
    "pixmap = QPixmap.fromImage(QImg)\n",
    "x_0.lb_0.setPixmap(pixmap)\n",
    "\n",
    "\n",
    "#* Input \n",
    "re = QRegExp('[a-zA-Z,]+$')\n",
    "re_validato = QRegExpValidator(re)\n",
    "x_0.customLabel = QLineEdit(x_0)\n",
    "x_0.customLabel.setValidator(re_validato)\n",
    "\n",
    "x_0.customLabel.move(x_0.lb_0.x()+50, x_0.lb_0.y()+20)\n",
    "x_0.customLabel.resize(510, 20)\n",
    "x_0.customLabel.setText(\"Please input your custom labels to get zero-shot detection! [separated by',']\")\n",
    "\n",
    "def textchange(text):\n",
    "  x_0.currentText = text\n",
    "  #//print(x.currentText)\n",
    "x_0.customLabel.textChanged.connect(textchange)\n",
    "\n",
    "\n",
    "  \n",
    "#* As Enter\n",
    "x_0.bt = QPushButton(x_0)\n",
    "x_0.bt.resize(30, 30)\n",
    "x_0.bt.setStyleSheet(\"background-color:transparent\")\n",
    "\n",
    "x_0.bt.setIcon(QIcon(\"icon/icons8-enter-mac-key-90.png\"))\n",
    "x_0.bt.setIconSize(QSize(x_0.bt.size()))\n",
    "x_0.bt.move(x_0.customLabel.x()+520, x_0.lb_0.y()+15)\n",
    "\n",
    "## Click this button ##\n",
    "#######################\n",
    "def clickInputCustomLabel():\n",
    "  x_0.msg_label = QMessageBox(x_0)\n",
    "  x_0.msg_label.setText(\"Get custom Labels :  \\n  \" + x_0.currentText)\n",
    "  x_0.msg_label.setIcon(QMessageBox.Information)\n",
    "  #//print(x.currentText)\n",
    "  x_0.msg_label.show()\n",
    "  cusLabel = x_0.currentText.split(',')\n",
    "  x_0.category_name_string = ';'.join(cusLabel) ## + background\n",
    "  print(\"???\")\n",
    "x_0.bt.clicked.connect(clickInputCustomLabel)\n",
    "\n",
    "print(\"Have Button 2\")\n",
    "x_0.bt_2 = QPushButton(x_0)\n",
    "x_0.bt_2.resize(30, 30)\n",
    "x_0.bt_2.setStyleSheet(\"background-color:transparent\")\n",
    "x_0.bt_2.setIcon(QIcon(\"icon/play.png\"))\n",
    "x_0.bt_2.setIconSize(QSize(x_0.bt_2.size()))\n",
    "x_0.bt_2.move(x_0.bt.x()+40, x_0.bt.y())  \n",
    "\n",
    "def StartDetecting():\n",
    "  print(\"Start Detecting\")\n",
    "  max_boxes_to_draw = 5 #@param {type:\"integer\"}\n",
    "  nms_threshold = 0.6 #@param {type:\"slider\", min:0, max:0.9, step:0.05} -- Non Maximum Suppression\n",
    "  min_rpn_score_thresh = 0.9  #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "  min_box_area = 220 #@param {type:\"slider\", min:0, max:10000, step:1.0}\n",
    "\n",
    "  # image_path, \n",
    "  # category_name_string, \n",
    "  #numbered_categories = [{'name': str(idx), 'id': idx,} for idx in range(50)]\n",
    "  # print(numbered_categories)  # [{'name': '0', 'id': 0}, {, ..., {'name': '49', 'id': 49}]\n",
    "  #numbered_category_indices = {cat['id']: cat for cat in numbered_categories}\n",
    "  # print(numbered_category_indices)  # 0: {'name': '0', 'id': 0}, ..., 49: {'name': '49', 'id': 49}}\n",
    "  category_names = [x.strip() for x in x_0.category_name_string.split(';')]\n",
    "  category_names = ['background'] + category_names\n",
    "  categories = [{'name': item, 'id': idx+1,} for idx, item in enumerate(category_names)]\n",
    "  category_indices = {cat['id']: cat for cat in categories}  ## The case in Helper Function\n",
    "  fig_size_h = min(max(5, int(len(category_names) / 2.5) ), 10)\n",
    "\n",
    "  roi_boxes, roi_scores, detection_boxes, scores_unused, box_outputs, detection_masks, visual_features, image_info \\\n",
    "  = session.run(\n",
    "      ['RoiBoxes:0', 'RoiScores:0', '2ndStageBoxes:0', '2ndStageScoresUnused:0', 'BoxOutputs:0', 'MaskOutputs:0', 'VisualFeatOutputs:0', 'ImageInfo:0'], # fetches\n",
    "      feed_dict={'Placeholder:0': [image_path, ] }\n",
    "    )\n",
    "\n",
    "  roi_boxes = np.squeeze(roi_boxes, axis=0)  # squeeze\n",
    "  roi_scores = np.squeeze(roi_scores, axis=0)\n",
    "  detection_boxes = np.squeeze(detection_boxes, axis=(0, 2))\n",
    "  scores_unused = np.squeeze(scores_unused, axis=0)\n",
    "  box_outputs = np.squeeze(box_outputs, axis=0)\n",
    "  detection_masks = np.squeeze(detection_masks, axis=0)\n",
    "  visual_features = np.squeeze(visual_features, axis=0)\n",
    "  image_info = np.squeeze(image_info, axis=0)  # obtain image info\n",
    "\n",
    "  image_scale = np.tile(image_info[2:3, :], (1, 2))\n",
    "  image_height = int(image_info[0, 0])\n",
    "  image_width = int(image_info[0, 1])\n",
    "  rescaled_detection_boxes = detection_boxes / image_scale # rescale\n",
    "\n",
    "  ## Read image\n",
    "  image = np.asarray(Image.open(open(image_path, 'rb')).convert(\"RGB\"))\n",
    "  assert image_height == image.shape[0]\n",
    "  assert image_width == image.shape[1]\n",
    "\n",
    "\n",
    "  ## Apply non-maximum suppression to detected boxes with nms threshold.\n",
    "  nmsed_indices = nms(detection_boxes, roi_scores, thresh=nms_threshold)\n",
    "  ## Compute RPN box size.\n",
    "  box_sizes = (rescaled_detection_boxes[:, 2] - rescaled_detection_boxes[:, 0]) * (rescaled_detection_boxes[:, 3] - rescaled_detection_boxes[:, 1])\n",
    "  ## Filter out invalid rois (nms-okay rois)\n",
    "  valid_indices = np.where(\n",
    "      np.logical_and(\n",
    "                    np.isin(np.arange(len(roi_scores), dtype=np.int), nmsed_indices),\n",
    "                    np.logical_and(\n",
    "                                  np.logical_not(np.all(roi_boxes == 0., axis=-1)),\n",
    "                                  np.logical_and(\n",
    "                                                roi_scores >= min_rpn_score_thresh,\n",
    "                                                box_sizes > min_box_area \n",
    "                                                ))))[0]\n",
    "  print('number of valid indices', len(valid_indices))\n",
    "\n",
    "  detection_roi_scores = roi_scores[valid_indices][:max_boxes_to_draw, ...]\n",
    "  detection_boxes = detection_boxes[valid_indices][:max_boxes_to_draw, ...]\n",
    "  detection_masks = detection_masks[valid_indices][:max_boxes_to_draw, ...]\n",
    "  #### UI ####\n",
    "  isSaved = np.zeros(detection_roi_scores.shape)\n",
    "  detection_visual_feat = visual_features[valid_indices][:max_boxes_to_draw, ...]  ## Get Detection Visual Feat\n",
    "  rescaled_detection_boxes = rescaled_detection_boxes[valid_indices][:max_boxes_to_draw, ...]  ## Get Rescaled Detection Boxes \n",
    "\n",
    "  text_features = build_text_embedding(categories)\n",
    "    \n",
    "  raw_scores = detection_visual_feat.dot(text_features.T) ## From visual features\n",
    "  if FLAGS.use_softmax:\n",
    "    scores_all = softmax(FLAGS.temperature * raw_scores, axis=-1)\n",
    "  else:\n",
    "    scores_all = raw_scores\n",
    "\n",
    "  indices = np.argsort(-np.max(scores_all, axis=1))  # Results are ranked by scores\n",
    "  indices_fg = np.array([i for i in indices if np.argmax(scores_all[i]) != 0])\n",
    "\n",
    "  ymin, xmin, ymax, xmax = np.split(rescaled_detection_boxes, 4, axis=-1)\n",
    "  processed_boxes = np.concatenate([xmin, ymin, xmax - xmin, ymax - ymin], axis=-1)\n",
    "  segmentations = paste_instance_masks(detection_masks, processed_boxes, image_height, image_width)\n",
    "  if len(indices_fg) == 0:\n",
    "    print('ViLD does not detect anything belong to the given category')\n",
    "  else:\n",
    "    #print(isSaved[indices_fg])\n",
    "    #print(rescaled_detection_boxes[indices_fg])\n",
    "    image_with_detections = \\\n",
    "      visualize_boxes_and_labels_on_image_array( \\\n",
    "        np.array(image),\n",
    "        rescaled_detection_boxes[indices_fg],\n",
    "        valid_indices[:max_boxes_to_draw][indices_fg],\n",
    "        detection_roi_scores[indices_fg],   \n",
    "        category_indices, #numbered_category_indices,\n",
    "        user=isSaved[indices_fg], \n",
    "        instance_masks=segmentations[indices_fg],\n",
    "        use_normalized_coordinates=False,\n",
    "        max_boxes_to_draw=max_boxes_to_draw, min_score_thresh=min_rpn_score_thresh,\n",
    "        skip_scores=False, skip_labels=False,\n",
    "      )\n",
    "      #//msg = QMessageBox()\n",
    "      #//msg.setIcon(QMessageBox.Information)\n",
    "      #//msg.setStandardButtons(QMessageBox.Ok | QMessageBox.Cancel)\n",
    "      #//retval = msg.exec_()\n",
    "      \n",
    "    plt.figure(figsize=overall_fig_size)\n",
    "    plt.imshow(image_with_detections)\n",
    "    plt.axis('off')\n",
    "    plt.savefig('tmp/result.png')\n",
    "    plt.title('Detected objects and RPN scores')\n",
    "    plt.show()\n",
    "    print(\"ENDING\")\n",
    "    #x.lb.setAlignment(Qt.AlignmentFlag.AlignLeft|Qt.AlignmentFlag.AlignTop)\n",
    "    #x.lb = QLabel(x) \n",
    "    #x.lb.setScaledContents(True)\n",
    "    #x.lb.setGeometry(QRect(0, 0, 1300, 900)) # With Padding\n",
    "    #print(x.lb.getContentsMargins())\n",
    "    #x.lb.setContentsMargins(0,0,0,0)\n",
    "    \n",
    "    #?x.deleteLater()\n",
    "    #?y = QWidget()\n",
    "\n",
    "    #x_0.lb_0.deleteLater()\n",
    "    #x_0.lb_0.hide()\n",
    "    #x_0.customLabel.deleteLater()\n",
    "    #x_0.customLabel.hide()\n",
    "    #x_0.bt.deleteLater()\n",
    "    #x_0.bt.hide()\n",
    "    #x_0.bt_2.deleteLater()\n",
    "    #x_0.bt_2.hide()\n",
    "    \n",
    "    #x_0.deleteLater()\n",
    "    x_0.update()\n",
    "    #x_0.hide()\n",
    "    #x_0.close()\n",
    "    '''\n",
    "    x_0.setWindowTitle(\"Update\")\n",
    "    x_0.llbb = MyLabel(x_0)\n",
    "    \n",
    "    img = cv2.imread(\"tmp/ui.jpg\")\n",
    "    height, width, bytesPerComponent = img.shape\n",
    "    x_0.resize(width, height)\n",
    "    x_0.llbb.setGeometry(QRect(0, 0, width, height))\n",
    "    x_0.move(int((QDesktopWidget().screenGeometry().width()-x_0.width())/2),int((QDesktopWidget().screenGeometry().height()-x_0.height())/2))\n",
    "    bytesPerLine = 3 * width\n",
    "    cv2.cvtColor(img, cv2.COLOR_BGR2RGB, img)\n",
    "    QImg = QImage(img.data, width, height, bytesPerLine, QImage.Format_RGB888)\n",
    "    pixmap = QPixmap.fromImage(QImg)\n",
    "    x_0.llbb.setPixmap(pixmap)\n",
    "    x_0.llbb.setCursor(Qt.CrossCursor)\n",
    "    x_0.update()\n",
    "    x_0.show()\n",
    "    '''\n",
    "    \n",
    "    x_1 = QMainWindow(x_0)\n",
    "    x_1.setWindowTitle('Detected objects. Zero-shot ends.')\n",
    "    x_1.lb = MyLabel(x_1) \n",
    "    #//img = cv2.imread(\"examples/test.jpg\")\n",
    "    img = cv2.imread(\"tmp/ui.jpg\")\n",
    "    \n",
    "    height, width, bytesPerComponent = img.shape\n",
    "    x_1.resize(width, height)\n",
    "    x_1.lb.setGeometry(QRect(0, 0, width, height))\n",
    "    x_1.move(int((QDesktopWidget().screenGeometry().width()-x_1.width())/2),int((QDesktopWidget().screenGeometry().height()-x_1.height())/2))\n",
    "    bytesPerLine = 3 * width\n",
    "    \n",
    "    cv2.cvtColor(img, cv2.COLOR_BGR2RGB, img)\n",
    "    \n",
    "    QImg = QImage(img.data, width, height, bytesPerLine, QImage.Format_RGB888)\n",
    "    pixmap = QPixmap.fromImage(QImg)\n",
    "    \n",
    "    x_1.lb.setPixmap(pixmap)\n",
    "    x_1.lb.setCursor(Qt.CrossCursor)\n",
    "    #//x.lb.setStyleSheet('background-color:green;border:0;margin:0;padding:0;')\n",
    "    #//print(\"OK\")\n",
    "    #//x_0.x_1.update()\n",
    "    x_1.show()\n",
    "\n",
    "    print(\"x_1 Built.\")\n",
    "    \n",
    "x_0.bt_2.clicked.connect(StartDetecting)\n",
    "\n",
    "\n",
    "x_0.bt_3 = QPushButton(x_0)\n",
    "x_0.bt_3.resize(25, 25)\n",
    "x_0.bt_3.setIcon(QIcon(\"icon/icons8-image-document-64\"))\n",
    "x_0.bt_3.setStyleSheet(\"background-color:transparent\")\n",
    "x_0.bt_3.setIconSize(QSize(x_0.bt_3.size()))\n",
    "x_0.bt_3.move(x_0.bt_2.x()+40, x_0.lb_0.y()+15)\n",
    "\n",
    "def clickChangeImage():\n",
    "  print(\"Clicked!\")\n",
    "  QFDialog = QFileDialog()\n",
    "  image_choose, _ = QFDialog.getOpenFileName(x_0, \n",
    "                                \"Choose\",\n",
    "                                 os.getcwd()+\"/examples\",\n",
    "                                 \"All Files (*);;Image Files (*.jpg)\")\n",
    "  if image_choose == \"\":\n",
    "    print(\"NO IMAGE CHOSEN!\")\n",
    "    return\n",
    "  print(\"New Iamge: \", image_choose, \"\\n\")\n",
    "  image_path = './examples/'+ image_choose\n",
    "  print(image_path) \n",
    "x_0.bt_3.clicked.connect(clickChangeImage)\n",
    "\n",
    "x_0.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permitted Detection counts: 0\n"
     ]
    }
   ],
   "source": [
    "#* ////////////////////////////  Plot //////////////////////////////////\n",
    "cnt = 0\n",
    "raw_image = np.array(image)\n",
    "n_boxes = rescaled_detection_boxes.shape[0]\n",
    "\n",
    "for anno_idx in indices[0:int(n_boxes)]:\n",
    "  rpn_score = detection_roi_scores[anno_idx]\n",
    "  bbox = rescaled_detection_boxes[anno_idx]  ## Get BBox\n",
    "  scores = scores_all[anno_idx]\n",
    "  showOrNot = isSaved[anno_idx]\n",
    "  #//print(showOrNot)\n",
    "  \n",
    "  if np.argmax(scores) == 0:\n",
    "    continue\n",
    "  ## UI ##\n",
    "  if showOrNot==0 or showOrNot==2:\n",
    "    continue\n",
    "  \n",
    "  y1, x1, y2, x2 = int(np.floor(bbox[0])), int(np.floor(bbox[1])), int(np.ceil(bbox[2])), int(np.ceil(bbox[3]))\n",
    "  img_w_mask = plot_mask(mask_color, alpha, raw_image, segmentations[anno_idx])\n",
    "  crop_w_mask = img_w_mask[y1:y2, x1:x2, :]\n",
    "\n",
    "  #fig, axs = plt.subplots(1, 4, figsize=(fig_size_w, fig_size_h), gridspec_kw={'width_ratios': [3, 1, 1, 2]}, constrained_layout=True)\n",
    "  fig, axs = plt.subplots(\n",
    "    1, 4, figsize=(fig_size_w, fig_size_h),\n",
    "    gridspec_kw={'width_ratios': [3, 1, 1, 2]}, \n",
    "    constrained_layout=True\n",
    "  )\n",
    "\n",
    "  # Draw bounding box.\n",
    "  rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=line_thickness, edgecolor='r', facecolor='none')\n",
    "  axs[0].add_patch(rect)\n",
    "  axs[0].set_xticks([])\n",
    "  axs[0].set_yticks([])\n",
    "  axs[0].set_title(f'bbox: {y1, x1, y2, x2}   area: {(y2 - y1) * (x2 - x1)}   rpn score: {rpn_score:.4f}') ## Risk Priority Number (RPN)\n",
    "  axs[0].imshow(raw_image)\n",
    "\n",
    "  #! category_names still not global\n",
    "  # Draw image in a cropped region.\n",
    "  crop = np.copy(raw_image[y1:y2, x1:x2, :])\n",
    "  axs[1].set_xticks([])\n",
    "  axs[1].set_yticks([])\n",
    "  axs[1].set_title(f'predicted: {category_names[np.argmax(scores)]}')\n",
    "  axs[1].imshow(crop)\n",
    "  # Draw segmentation inside a cropped region.\n",
    "  axs[2].set_xticks([])\n",
    "  axs[2].set_yticks([])\n",
    "  axs[2].set_title('mask')\n",
    "  axs[2].imshow(crop_w_mask)\n",
    "\n",
    "  # Draw category scores.\n",
    "  fontsize = max(min(fig_size_h / float(len(category_names)) * 45, 20), 8)\n",
    "  for cat_idx in range(len(category_names)):\n",
    "    axs[3].barh(cat_idx, scores[cat_idx], \n",
    "                color='orange' if scores[cat_idx] == max(scores) else 'blue')\n",
    "  axs[3].invert_yaxis()\n",
    "  axs[3].set_axisbelow(True)\n",
    "  axs[3].set_xlim(0, 1)\n",
    "  plt.xlabel(\"confidence score\")\n",
    "  axs[3].set_yticks(range(len(category_names)))\n",
    "  axs[3].set_yticklabels(category_names, fontdict={'fontsize': fontsize})\n",
    "  \n",
    "  \n",
    "  f = plt.gcf()\n",
    "  f.savefig('result/result_{id}.jpg'.format(id=anno_idx), bbox_inches=\"tight\") # png could be transparent\n",
    "  plt.show()\n",
    "  f.clear()\n",
    "    \n",
    "  cnt += 1\n",
    "print('Permitted Detection counts:', cnt)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86c8192e3b52dd5b61cf7d633d9cde44ca62fe25113112c353ce106148b537f5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('zeroShot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
