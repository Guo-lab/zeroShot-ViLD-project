{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#@title Import libraries\n",
    "from easydict import EasyDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "\n",
    "import collections\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "from scipy.special import softmax\n",
    "import yaml\n",
    "\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "import cv2\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define hyperparameters\n",
    "FLAGS = {\n",
    "    'prompt_engineering': True,\n",
    "    'this_is': True,\n",
    "    \n",
    "    'temperature': 100.0,\n",
    "    'use_softmax': False,\n",
    "}\n",
    "FLAGS = EasyDict(FLAGS)\n",
    "\n",
    "\n",
    "# Global matplotlib settings\n",
    "SMALL_SIZE = 16#10\n",
    "MEDIUM_SIZE = 18#12\n",
    "BIGGER_SIZE = 20#14\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "\n",
    "# Parameters for drawing figure.\n",
    "display_input_size = (10, 10)\n",
    "overall_fig_size = (18, 24)\n",
    "\n",
    "line_thickness = 2\n",
    "fig_size_w = 35\n",
    "# fig_size_h = min(max(5, int(len(category_names) / 2.5) ), 10)\n",
    "mask_color =   'red'\n",
    "alpha = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build text embeddings\n",
    "\n",
    "We use the CLIP model from OpenAI: https://github.com/openai/CLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article(name):  #% A/An\n",
    "  return 'an' if name[0] in 'aeiou' else 'a'\n",
    "\n",
    "\n",
    "def processed_name(name, rm_dot=False):  #% '_' for lvis, '/' for obj365\n",
    "  res = name.replace('_', ' ').replace('/', ' or ').lower()\n",
    "  if rm_dot:\n",
    "    res = res.rstrip('.')\n",
    "  return res\n",
    "\n",
    "\n",
    "# //////////////////////////////////////////////////////////////\n",
    "single_template = [  #% Only one\n",
    "    'a photo of {article} {}.'\n",
    "]\n",
    "# //////////////////////////////////////////////////////////////\n",
    "# //////////////////////////////////////////////////////////////\n",
    "multiple_templates = [\n",
    "    'There is {article} {} in the scene.',\n",
    "    'There is the {} in the scene.',\n",
    "    'a photo of {article} {} in the scene.',\n",
    "    'a photo of the {} in the scene.',\n",
    "    'a photo of one {} in the scene.',\n",
    "\n",
    "    'itap of {article} {}.',\n",
    "    'itap of my {}.',  # itap: I took a picture of\n",
    "    'itap of the {}.',\n",
    "    \n",
    "    'a photo of {article} {}.',\n",
    "    'a photo of my {}.',\n",
    "    'a photo of the {}.',\n",
    "    'a photo of one {}.',\n",
    "    'a photo of many {}.',\n",
    "\n",
    "    'a good photo of {article} {}.',\n",
    "    'a good photo of the {}.',\n",
    "    'a bad photo of {article} {}.',\n",
    "    'a bad photo of the {}.',\n",
    "    'a photo of a nice {}.',\n",
    "    'a photo of the nice {}.',\n",
    "    'a photo of a cool {}.',\n",
    "    'a photo of the cool {}.',\n",
    "    'a photo of a weird {}.',\n",
    "    'a photo of the weird {}.',\n",
    "\n",
    "    'a photo of a small {}.',\n",
    "    'a photo of the small {}.',\n",
    "    'a photo of a large {}.',\n",
    "    'a photo of the large {}.',\n",
    "\n",
    "    'a photo of a clean {}.',\n",
    "    'a photo of the clean {}.',\n",
    "    'a photo of a dirty {}.',\n",
    "    'a photo of the dirty {}.',\n",
    "\n",
    "    'a bright photo of {article} {}.',\n",
    "    'a bright photo of the {}.',\n",
    "    'a dark photo of {article} {}.',\n",
    "    'a dark photo of the {}.',\n",
    "\n",
    "    'a photo of a hard to see {}.',\n",
    "    'a photo of the hard to see {}.',\n",
    "    'a low resolution photo of {article} {}.',\n",
    "    'a low resolution photo of the {}.',\n",
    "    'a cropped photo of {article} {}.',\n",
    "    'a cropped photo of the {}.',\n",
    "    'a close-up photo of {article} {}.',\n",
    "    'a close-up photo of the {}.',\n",
    "    'a jpeg corrupted photo of {article} {}.',\n",
    "    'a jpeg corrupted photo of the {}.',\n",
    "    'a blurry photo of {article} {}.',\n",
    "    'a blurry photo of the {}.',\n",
    "    'a pixelated photo of {article} {}.',\n",
    "    'a pixelated photo of the {}.',\n",
    "\n",
    "    'a black and white photo of the {}.',\n",
    "    'a black and white photo of {article} {}.',\n",
    "\n",
    "    'a plastic {}.',\n",
    "    'the plastic {}.',\n",
    "\n",
    "    'a toy {}.',\n",
    "    'the toy {}.',\n",
    "    'a plushie {}.',\n",
    "    'the plushie {}.',\n",
    "    'a cartoon {}.',\n",
    "    'the cartoon {}.',\n",
    "\n",
    "    'an embroidered {}.',\n",
    "    'the embroidered {}.',\n",
    "\n",
    "    'a painting of the {}.',\n",
    "    'a painting of a {}.',\n",
    "]\n",
    "# //////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.available_models()\n",
    "# print(clip.available_models()) # ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "# print(model) # visual() & transform()\n",
    "# print(preprocess) # Compose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build text embedding function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_embedding(categories):\n",
    "  if FLAGS.prompt_engineering:  ## prompt engineering => some templates\n",
    "    templates = multiple_templates\n",
    "  else:\n",
    "    templates = single_template\n",
    "\n",
    "  with torch.no_grad():\n",
    "    all_text_embeddings = []  ## All Text Embeddings\n",
    "    print('Building text embeddings...')\n",
    "    \n",
    "    for category in tqdm(categories):\n",
    "      texts = [\n",
    "        template.format(processed_name(category['name'], rm_dot=True),\n",
    "                        article=article(category['name']))\n",
    "        for template in templates\n",
    "      ]\n",
    "      if FLAGS.this_is:\n",
    "        texts = [\n",
    "                 'This is ' + text if text.startswith('a') or text.startswith('the') else text \n",
    "                 for text in texts\n",
    "                 ]\n",
    "      texts = clip.tokenize(texts) #@ Returns a LongTensor containing tokenized sequences of given text input(s).\n",
    "      #TODO print(\"texts[tokenized sequences of given text inputs]: \", texts) ## This can be used as the input to the model.\n",
    "      if torch.cuda.is_available():\n",
    "        texts = texts.cuda()\n",
    "      \n",
    "      #@ Given a batch of text tokens, returns the text features encoded by the language portion of the CLIP model.  \n",
    "      text_embeddings = model.encode_text(texts) #embed with text encoder \n",
    "      text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "      text_embedding = text_embeddings.mean(dim=0)\n",
    "      text_embedding /= text_embedding.norm()\n",
    "      all_text_embeddings.append(text_embedding)\n",
    "    ## FOR ENDING  \n",
    "    all_text_embeddings = torch.stack(all_text_embeddings, dim=1)\n",
    "    if torch.cuda.is_available():\n",
    "      all_text_embeddings = all_text_embeddings.cuda()\n",
    "  ## WITH ENDING  \n",
    "  return all_text_embeddings.cpu().numpy().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ViLD model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/fc/n5827zt91vb65jnqcsgpv5cr0000gn/T/ipykernel_34958/2048092235.py:9: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 20:25:34.966178: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The passed save_path is not a valid checkpoint: ./image_path_v2/variables/variables",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/gsq/Desktop/zero_shot_object_detection/my_vild_demo.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gsq/Desktop/zero_shot_object_detection/my_vild_demo.ipynb#ch0000008?line=2'>3</a>\u001b[0m saved_model_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./image_path_v2\u001b[39m\u001b[39m'\u001b[39m \u001b[39m#@param {type:\"string\"}\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gsq/Desktop/zero_shot_object_detection/my_vild_demo.ipynb#ch0000008?line=4'>5</a>\u001b[0m \u001b[39m#! Session would be used in main()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gsq/Desktop/zero_shot_object_detection/my_vild_demo.ipynb#ch0000008?line=5'>6</a>\u001b[0m \u001b[39m#@session: to restore the variables\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gsq/Desktop/zero_shot_object_detection/my_vild_demo.ipynb#ch0000008?line=6'>7</a>\u001b[0m \u001b[39m#@['serve']: correspond to the tags used, saving the variables using the SavedModel save() API\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gsq/Desktop/zero_shot_object_detection/my_vild_demo.ipynb#ch0000008?line=7'>8</a>\u001b[0m \u001b[39m#@ Returns:  protocol buffer loaded in the provided session\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gsq/Desktop/zero_shot_object_detection/my_vild_demo.ipynb#ch0000008?line=8'>9</a>\u001b[0m _ \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49msaved_model\u001b[39m.\u001b[39;49mloader\u001b[39m.\u001b[39;49mload(session, [\u001b[39m'\u001b[39;49m\u001b[39mserve\u001b[39;49m\u001b[39m'\u001b[39;49m], saved_model_dir)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:357\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecated_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py?line=348'>349</a>\u001b[0m       _PRINTED_WARNING[func] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py?line=349'>350</a>\u001b[0m     logging\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py?line=350'>351</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mFrom \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m (from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) is deprecated and will be removed \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py?line=351'>352</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mInstructions for updating:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py?line=354'>355</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39min a future version\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m date \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mafter \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m date),\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py?line=355'>356</a>\u001b[0m         instructions)\n\u001b[0;32m--> <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py?line=356'>357</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py:340\u001b[0m, in \u001b[0;36mload\u001b[0;34m(sess, tags, export_dir, import_scope, **saver_kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py?line=277'>278</a>\u001b[0m \u001b[39m\"\"\"Loads the model from a SavedModel as specified by tags.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py?line=278'>279</a>\u001b[0m \n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py?line=279'>280</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py?line=336'>337</a>\u001b[0m \u001b[39m@end_compatibility\u001b[39;00m\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py?line=337'>338</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py?line=338'>339</a>\u001b[0m loader \u001b[39m=\u001b[39m SavedModelLoader(export_dir)\n\u001b[0;32m--> <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py?line=339'>340</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loader\u001b[39m.\u001b[39;49mload(sess, tags, import_scope, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msaver_kwargs)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py:500\u001b[0m, in \u001b[0;36mSavedModelLoader.load\u001b[0;34m(self, sess, tags, import_scope, **saver_kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py?line=496'>497</a>\u001b[0m \u001b[39mwith\u001b[39;00m sess\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mas_default():\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py?line=497'>498</a>\u001b[0m   saver, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_graph(sess\u001b[39m.\u001b[39mgraph, tags, import_scope,\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py?line=498'>499</a>\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msaver_kwargs)\n\u001b[0;32m--> <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py?line=499'>500</a>\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrestore_variables(sess, saver, import_scope)\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py?line=500'>501</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_init_ops(sess, tags, import_scope)\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py?line=501'>502</a>\u001b[0m meta_graph_def \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_meta_graph_def_from_tags(tags)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py:451\u001b[0m, in \u001b[0;36mSavedModelLoader.restore_variables\u001b[0;34m(self, sess, saver, import_scope)\u001b[0m\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py?line=447'>448</a>\u001b[0m   tf_logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mThe specified SavedModel has no variables; no \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py?line=448'>449</a>\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mcheckpoints were restored.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py?line=449'>450</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(saver, tf_saver\u001b[39m.\u001b[39mSaver):\n\u001b[0;32m--> <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py?line=450'>451</a>\u001b[0m   saver\u001b[39m.\u001b[39;49mrestore(sess, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variables_path)\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py?line=451'>452</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py?line=452'>453</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py?line=453'>454</a>\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mNo tf.train.Saver object was passed to the function \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py?line=454'>455</a>\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m`SavedModelLoader.restore_variables`. Since there are variables in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/saved_model/loader_impl.py?line=455'>456</a>\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m the graph, a saver is required.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/training/saver.py:1409\u001b[0m, in \u001b[0;36mSaver.restore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/training/saver.py?line=1406'>1407</a>\u001b[0m checkpoint_prefix \u001b[39m=\u001b[39m compat\u001b[39m.\u001b[39mas_text(save_path)\n\u001b[1;32m   <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/training/saver.py?line=1407'>1408</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m checkpoint_management\u001b[39m.\u001b[39mcheckpoint_exists_internal(checkpoint_prefix):\n\u001b[0;32m-> <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/training/saver.py?line=1408'>1409</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe passed save_path is not a valid checkpoint: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[1;32m   <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/training/saver.py?line=1409'>1410</a>\u001b[0m                    checkpoint_prefix)\n\u001b[1;32m   <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/training/saver.py?line=1411'>1412</a>\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mRestoring parameters from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, checkpoint_prefix)\n\u001b[1;32m   <a href='file:///Applications/anaconda3/envs/zeroShot/lib/python3.9/site-packages/tensorflow/python/training/saver.py?line=1412'>1413</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: The passed save_path is not a valid checkpoint: ./image_path_v2/variables/variables"
     ]
    }
   ],
   "source": [
    "session = tf.Session(graph=tf.Graph())\n",
    "\n",
    "saved_model_dir = './image_path_v2' #@param {type:\"string\"}\n",
    "\n",
    "#! Session would be used in main()\n",
    "#@session: to restore the variables\n",
    "#@['serve']: correspond to the tags used, saving the variables using the SavedModel save() API\n",
    "#@ Returns:  protocol buffer loaded in the provided session\n",
    "_ = tf.saved_model.loader.load(session, ['serve'], saved_model_dir) # Restoring parameters from ./image_path_v2/variables/variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functioins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbered_categories = [{'name': str(idx), 'id': idx,} for idx in range(50)]\n",
    "# print(numbered_categories)  # [{'name': '0', 'id': 0}, {, ..., {'name': '49', 'id': 49}]\n",
    "numbered_category_indices = {cat['id']: cat for cat in numbered_categories}\n",
    "# print(numbered_category_indices)  # 0: {'name': '0', 'id': 0}, ..., 49: {'name': '49', 'id': 49}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Non Maximum Suppression\n",
    "#@ dets: [N, 4] \n",
    "#@ scores: [N,]\n",
    "#@ thresh[Float]: iou threshold.\n",
    "#@ max_dets[Int]\n",
    "def nms(dets, scores, thresh, max_dets=1000):\n",
    "  y1 = dets[:, 0]\n",
    "  x1 = dets[:, 1]\n",
    "  y2 = dets[:, 2]\n",
    "  x2 = dets[:, 3]\n",
    "\n",
    "  areas = (x2 - x1) * (y2 - y1)\n",
    "  order = scores.argsort()[::-1]\n",
    "\n",
    "  keep = []\n",
    "  while order.size > 0 and len(keep) < max_dets:\n",
    "    i = order[0]\n",
    "    keep.append(i)\n",
    "\n",
    "    xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "    yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "    xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "    yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "    w = np.maximum(0.0, xx2 - xx1)\n",
    "    h = np.maximum(0.0, yy2 - yy1)\n",
    "    intersection = w * h\n",
    "    overlap = intersection / (areas[i] + areas[order[1:]] - intersection + 1e-12)\n",
    "\n",
    "    inds = np.where(overlap <= thresh)[0]\n",
    "    order = order[inds + 1]\n",
    "  ## WHILE ENDING \n",
    "  return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Visualization\n",
    "import PIL.ImageColor as ImageColor\n",
    "import PIL.ImageDraw as ImageDraw\n",
    "import PIL.ImageFont as ImageFont\n",
    "\n",
    "STANDARD_COLORS = [  ## Color Choices\n",
    "    'AliceBlue', 'Chartreuse', 'Aqua', 'Aquamarine', 'Azure', 'Beige', 'Bisque', 'BlanchedAlmond', 'BlueViolet', 'BurlyWood', 'CadetBlue', 'AntiqueWhite', 'Chocolate', 'Coral', \n",
    "    'CornflowerBlue', 'Cornsilk', 'Cyan', 'DarkCyan', 'DarkGoldenRod', 'DarkGrey', 'DarkKhaki', 'DarkOrange', 'DarkOrchid', 'DarkSalmon', 'DarkSeaGreen', 'DarkTurquoise', 'DarkViolet', \n",
    "    'DeepPink', 'DeepSkyBlue', 'DodgerBlue', 'FloralWhite', 'ForestGreen', 'Fuchsia', 'Gainsboro', 'GhostWhite', 'Gold', 'GoldenRod', 'Salmon', 'Tan', 'HoneyDew', 'HotPink', 'Ivory',\n",
    "    'Khaki', 'Lavender', 'LavenderBlush', 'LawnGreen', 'LemonChiffon', 'LightBlue', 'LightCoral', 'LightCyan', 'LightGoldenRodYellow', 'LightGray', 'LightGrey', 'LightGreen', 'LightPink',\n",
    "    'LightSalmon', 'LightSeaGreen', 'LightSkyBlue', 'LightSlateGray', 'LightSlateGrey', 'LightSteelBlue', 'LightYellow', 'Lime', 'LimeGreen', 'Linen', 'Magenta', 'MediumAquaMarine', \n",
    "    'MediumOrchid', 'MediumPurple', 'MediumSeaGreen', 'MediumSlateBlue', 'MediumSpringGreen', 'MediumTurquoise', 'MediumVioletRed', 'MintCream', 'MistyRose', 'Moccasin', 'NavajoWhite',\n",
    "    'OldLace', 'Olive', 'OliveDrab', 'Orange', 'Orchid', 'PaleGoldenRod', 'PaleGreen', 'PaleTurquoise', 'PaleVioletRed', 'PapayaWhip', 'PeachPuff', 'Peru', 'Pink', 'Plum', 'PowderBlue',\n",
    "    'Purple', 'RosyBrown', 'RoyalBlue', 'SaddleBrown', 'Green', 'SandyBrown', 'SeaGreen', 'SeaShell', 'Sienna', 'Silver', 'SkyBlue', 'SlateBlue', 'SlateGray', 'SlateGrey', 'Snow', \n",
    "    'SpringGreen', 'SteelBlue', 'GreenYellow', 'Teal', 'Thistle', 'Tomato', 'Turquoise', 'Violet', 'Wheat', 'White', 'WhiteSmoke', 'Yellow', 'YellowGreen'\n",
    "]\n",
    "\n",
    "#* #########################################################\n",
    "#@ Draw_boxes function\n",
    "#@ image: a PIL.Image object.\n",
    "#@ display_str_list: list of strings to display in box (out of range => displaying below the box)\n",
    "#@ use_normalized_coordinates: True (default) => treat coordinates as relative to the image. Otherwise => absolute.\n",
    "def draw_bounding_box_on_image(image, ymin, xmin, ymax, xmax,\n",
    "                               color='red', thickness=4,\n",
    "                               display_str_list=(),\n",
    "                               use_normalized_coordinates=True):\n",
    "  draw = ImageDraw.Draw(image)\n",
    "  im_width, im_height = image.size\n",
    "  if use_normalized_coordinates:\n",
    "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width, ymin * im_height, ymax * im_height)\n",
    "  else:\n",
    "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
    "  draw.line([(left, top), (left, bottom), (right, bottom), (right, top), (left, top)], width=thickness, fill=color)\n",
    "  \n",
    "  try:\n",
    "    font = ImageFont.truetype('arial.ttf', 24)\n",
    "  except IOError:\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "  #@ Displayed string position adjustment \n",
    "  display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n",
    "  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)   # display_str has top and bottom margin_(0.05x).\n",
    "  if top > total_display_str_height:\n",
    "    text_bottom = top\n",
    "  else:\n",
    "    text_bottom = bottom + total_display_str_height\n",
    "  for display_str in display_str_list[::-1]:  ## Reverse str position\n",
    "    text_left = min(5, left)\n",
    "    text_width, text_height = font.getsize(display_str)\n",
    "    margin = np.ceil(0.05 * text_height)\n",
    "    draw.rectangle([(left, text_bottom - text_height - 2 * margin), (left + text_width, text_bottom)], fill=color)\n",
    "    draw.text( (left + margin, text_bottom - text_height - margin),\n",
    "               display_str, \n",
    "               fill='black',\n",
    "               font=font)\n",
    "    text_bottom -= text_height - 2 * margin\n",
    "\n",
    "\n",
    "\n",
    "#* #########################################################\n",
    "#@ Draw_box on image (numpy array form) {CALL draw_bounding_box_on_image()}\n",
    "#@ image: a numpy array with shape [height, width, 3].\n",
    "def draw_bounding_box_on_image_array(image, ymin, xmin, ymax, xmax,\n",
    "                                     color='red', thickness=4,\n",
    "                                     display_str_list=(),\n",
    "                                     use_normalized_coordinates=True):\n",
    "  image_pil = Image.fromarray(np.uint8(image)).convert('RGB')\n",
    "  draw_bounding_box_on_image(image_pil, ymin, xmin, ymax, xmax, color, thickness, display_str_list, use_normalized_coordinates)\n",
    "  np.copyto(image, np.array(image_pil))  # image flush\n",
    "\n",
    "\n",
    "\n",
    "#* #########################################################\n",
    "#@ Draw mask on image(numpy array)\n",
    "#@ image: uint8 numpy array with shape [img_height, img_height, 3]\n",
    "#@ mask: uint8 numpy array in shape [img_height, img_height] with values [0 / 1].\n",
    "def draw_mask_on_image_array(image, mask, color='red', alpha=0.4):\n",
    "  if image.dtype != np.uint8:\n",
    "    raise ValueError('`image` not of type np.uint8')\n",
    "  if mask.dtype != np.uint8:\n",
    "    raise ValueError('`mask` not of type np.uint8')\n",
    "  if np.any(np.logical_and(mask != 1, mask != 0)):\n",
    "    raise ValueError('`mask` elements should be in [0, 1]')\n",
    "  if image.shape[:2] != mask.shape:\n",
    "    raise ValueError('The image has spatial dimensions %s, but the mask has dimensions %s' % (image.shape[:2], mask.shape))\n",
    "  \n",
    "  rgb = ImageColor.getrgb(color)\n",
    "  pil_image = Image.fromarray(image)\n",
    "\n",
    "  solid_color = np.expand_dims(np.ones_like(mask), axis=2) * np.reshape(list(rgb), [1, 1, 3])\n",
    "  pil_solid_color = Image.fromarray(np.uint8(solid_color)).convert('RGBA')\n",
    "  pil_mask = Image.fromarray(np.uint8(255.0*alpha*mask)).convert('L')\n",
    "  pil_image = Image.composite(pil_solid_color, pil_image, pil_mask)\n",
    "  np.copyto(image, np.array(pil_image.convert('RGB')))\n",
    "#* #########################################################\n",
    "#@ Plot the mask \n",
    "def plot_mask(color, alpha, original_image, mask):  #! Simialr to draw_mask_on_image_array()\n",
    "  rgb = ImageColor.getrgb(color)\n",
    "  pil_image = Image.fromarray(original_image)\n",
    "\n",
    "  solid_color = np.expand_dims(np.ones_like(mask), axis=2) * np.reshape(list(rgb), [1, 1, 3])\n",
    "  pil_solid_color = Image.fromarray(np.uint8(solid_color)).convert('RGBA')\n",
    "  pil_mask = Image.fromarray(np.uint8(255.0*alpha*mask)).convert('L')\n",
    "  pil_image = Image.composite(pil_solid_color, pil_image, pil_mask)\n",
    "  ## Maintain the original image\n",
    "  img_w_mask = np.array(pil_image.convert('RGB'))\n",
    "  return img_w_mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#* #########################################################\n",
    "#@ Visualize labeled boxes, scores\n",
    "#@ image: uint8 numpy array [img_height, img_width, 3]\n",
    "#@ boxes: numpy array [N, 4]\n",
    "#@ classes: numpy array [N]. Note that class indices are 1-based and match the keys in the label map.\n",
    "#@ scores: numpy array [N] or None(assumes plotted boxes are groundtruth boxes. Plot all boxes as black without classes/scores)\n",
    "#@ category_index: a dict [[category index `id`, category name `name`] keyed by category indices]\n",
    "#@\n",
    "#@ instance_masks: numpy array [N, image_height, image_width] with values [0/1]\n",
    "#@ instance_boundaries: numpy array [N, image_height, image_width] with values [0/1]\n",
    "#@\n",
    "#@ agnostic_mode[boolean]: class-agnostic mode_(display scores but ignore classes)\n",
    "def visualize_boxes_and_labels_on_image_array(image, boxes, classes, scores, category_index,\n",
    "                                              instance_masks=None, instance_boundaries=None,\n",
    "                                              use_normalized_coordinates=False,\n",
    "                                              max_boxes_to_draw=20, min_score_thresh=.5, #max number of boxes and  minimum score to visualize\n",
    "                                              agnostic_mode=False,\n",
    "                                              line_thickness=4,\n",
    "                                              groundtruth_box_visualization_color='black',\n",
    "                                              skip_scores=False, skip_labels=False,\n",
    "                                              mask_alpha=0.4,\n",
    "                                              plot_color=None,):\n",
    "  ## Create a display string(color) for each boxLoc, group any that correspond to one same loc.\n",
    "  #////////////////////////////////////////////////////////////\n",
    "  box_to_display_str_map = collections.defaultdict(list)\n",
    "  box_to_color_map = collections.defaultdict(str)\n",
    "  box_to_instance_masks_map = {}\n",
    "  box_to_score_map = {}\n",
    "  box_to_instance_boundaries_map = {}\n",
    "  \n",
    "  if not max_boxes_to_draw:\n",
    "    max_boxes_to_draw = boxes.shape[0]\n",
    "    \n",
    "  for i in range(min(max_boxes_to_draw, boxes.shape[0])):\n",
    "    if scores is None or scores[i] > min_score_thresh:\n",
    "      box = tuple(boxes[i].tolist())\n",
    "      if instance_masks is not None:\n",
    "        box_to_instance_masks_map[box] = instance_masks[i]\n",
    "      if instance_boundaries is not None:\n",
    "        box_to_instance_boundaries_map[box] = instance_boundaries[i]\n",
    "      if scores is None:\n",
    "        box_to_color_map[box] = groundtruth_box_visualization_color\n",
    "      \n",
    "      else:\n",
    "        display_str = ''\n",
    "        if not skip_labels:\n",
    "          if not agnostic_mode:\n",
    "            if classes[i] in list(category_index.keys()):\n",
    "              class_name = category_index[classes[i]]['name']\n",
    "            else:\n",
    "              class_name = 'N/A'\n",
    "            display_str = str(class_name)\n",
    "        ## SKIP_LABELS IF ENDING\n",
    "        if not skip_scores:\n",
    "          if not display_str:\n",
    "            display_str = '{}%'.format(int(100*scores[i]))\n",
    "          else:\n",
    "            float_score = (\"%.2f\" % scores[i]).lstrip('0')\n",
    "            display_str = '{}: {}'.format(display_str, float_score)\n",
    "          box_to_score_map[box] = int(100*scores[i])\n",
    "        ## SKIP_SCORES IF ENDING \n",
    "        box_to_display_str_map[box].append(display_str)\n",
    "        if plot_color is not None:\n",
    "          box_to_color_map[box] = plot_color\n",
    "        elif agnostic_mode:\n",
    "          box_to_color_map[box] = 'DarkOrange'\n",
    "        else:\n",
    "          box_to_color_map[box] = STANDARD_COLORS[\n",
    "              classes[i] % len(STANDARD_COLORS)]\n",
    "  \n",
    "  ## Handle the case when box_to_score_map is empty.\n",
    "  #////////////////////////////////////////////////////////////\n",
    "  if box_to_score_map:\n",
    "    box_color_iter = sorted(box_to_color_map.items(), key=lambda kv: box_to_score_map[kv[0]])\n",
    "  else:\n",
    "    box_color_iter = box_to_color_map.items()\n",
    "\n",
    "  ## Draw all boxes onto image.\n",
    "  #////////////////////////////////////////////////////////////\n",
    "  for box, color in box_color_iter:\n",
    "    ymin, xmin, ymax, xmax = box\n",
    "    if instance_masks is not None:\n",
    "      draw_mask_on_image_array(image, box_to_instance_masks_map[box], color=color, alpha=mask_alpha)\n",
    "    if instance_boundaries is not None:\n",
    "      draw_mask_on_image_array(image, box_to_instance_boundaries_map[box], color='red', alpha=1.0)\n",
    "    draw_bounding_box_on_image_array(image, ymin, xmin, ymax, xmax,\n",
    "                                     color=color, thickness=line_thickness,\n",
    "                                     display_str_list=box_to_display_str_map[box],\n",
    "                                     use_normalized_coordinates=use_normalized_coordinates)\n",
    "  return image   #@ Return uint8 numpy array [img_height, img_width, 3] with overlaid boxes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#* #########################################################\n",
    "#@ Generate the image segmentation results.\n",
    "#@ masks: numpy array [N, mask_height, mask_width] => the instance masks w.r.t. the `detected_boxes`.\n",
    "#@ detected_boxes: numpy array [N, 4] representing the reference bounding boxes.\n",
    "def paste_instance_masks(masks, detected_boxes, image_height, image_width):\n",
    "  def expand_boxes(boxes, scale):\n",
    "    \"\"\"Expands an array of boxes by a given scale.\"\"\"\n",
    "    # Reference: https://github.com/facebookresearch/Detectron/blob/master/detectron/utils/boxes.py#L227  # pylint: disable=line-too-long\n",
    "    # The `boxes` in the reference implementation is in [x1, y1, x2, y2] form, whereas `boxes` here is in [x1, y1, w, h] form\n",
    "    w_half = boxes[:, 2] * .5\n",
    "    h_half = boxes[:, 3] * .5\n",
    "    x_c = boxes[:, 0] + w_half\n",
    "    y_c = boxes[:, 1] + h_half\n",
    "\n",
    "    w_half *= scale\n",
    "    h_half *= scale\n",
    "\n",
    "    boxes_exp = np.zeros(boxes.shape)\n",
    "    boxes_exp[:, 0] = x_c - w_half\n",
    "    boxes_exp[:, 2] = x_c + w_half\n",
    "    boxes_exp[:, 1] = y_c - h_half\n",
    "    boxes_exp[:, 3] = y_c + h_half\n",
    "\n",
    "    return boxes_exp\n",
    "\n",
    "  # Reference: https://github.com/facebookresearch/Detectron/blob/master/detectron/core/test.py#L812  # pylint: disable=line-too-long\n",
    "  # To work around an issue with cv2.resize (it seems to automatically pad with repeated border values),\n",
    "  # we manually zero-pad the masks by 1 pixel prior to resizing back to the original image resolution. \n",
    "  # This prevents \"top hat\" artifacts. We therefore need to expand the reference boxes by an appropriate factor.\n",
    "  _, mask_height, mask_width = masks.shape\n",
    "  scale = max((mask_width + 2.0) / mask_width, (mask_height + 2.0) / mask_height)\n",
    "\n",
    "  ref_boxes = expand_boxes(detected_boxes, scale)  ## Expand the boxes\n",
    "  ref_boxes = ref_boxes.astype(np.int32)\n",
    "  padded_mask = np.zeros((mask_height + 2, mask_width + 2), dtype=np.float32)\n",
    "  segms = []\n",
    "  for mask_ind, mask in enumerate(masks):\n",
    "    im_mask = np.zeros((image_height, image_width), dtype=np.uint8)\n",
    "\n",
    "    padded_mask[1:-1, 1:-1] = mask[:, :]  # Process mask inside bounding boxes.\n",
    "\n",
    "    ref_box = ref_boxes[mask_ind, :]\n",
    "    w = ref_box[2] - ref_box[0] + 1\n",
    "    h = ref_box[3] - ref_box[1] + 1\n",
    "    w = np.maximum(w, 1)\n",
    "    h = np.maximum(h, 1)\n",
    "\n",
    "    mask = cv2.resize(padded_mask, (w, h))\n",
    "    mask = np.array(mask > 0.5, dtype=np.uint8)\n",
    "\n",
    "    x_0 = min(max(ref_box[0], 0), image_width)\n",
    "    x_1 = min(max(ref_box[2] + 1, 0), image_width)\n",
    "    y_0 = min(max(ref_box[1], 0), image_height)\n",
    "    y_1 = min(max(ref_box[3] + 1, 0), image_height)\n",
    "\n",
    "    im_mask[y_0:y_1, x_0:x_1] = mask[\n",
    "        (y_0 - ref_box[1]):(y_1 - ref_box[1]),\n",
    "        (x_0 - ref_box[0]):(x_1 - ref_box[0])\n",
    "    ]\n",
    "    segms.append(im_mask)\n",
    "\n",
    "  segms = np.array(segms)\n",
    "  assert masks.shape[0] == segms.shape[0]\n",
    "  \n",
    "  return segms   #@ Return numpy array [N, image_height, image_width] => the instance masks pasted on the image canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#\n",
    "# Main functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def display_image(path_or_array, size=(10, 10)):\n",
    "  if isinstance(path_or_array, str):\n",
    "    image = np.asarray(Image.open(open(image_path, 'rb')).convert(\"RGB\"))\n",
    "  else:\n",
    "    image = path_or_array\n",
    "  \n",
    "  plt.figure(figsize=size)\n",
    "  plt.imshow(image)\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(image_path, category_name_string, params):\n",
    "  #* /////////////////////////////////////////////////////////////////////\n",
    "  #* //////////  Preprocessing categories and get params  ////////////////\n",
    "  #///////////////////////////////////////////////////////////////////////\n",
    "  category_names = [x.strip() for x in category_name_string.split(';')]\n",
    "  category_names = ['background'] + category_names\n",
    "  categories = [{'name': item, 'id': idx+1,} for idx, item in enumerate(category_names)]\n",
    "  category_indices = {cat['id']: cat for cat in categories}  ## The case in Helper Function\n",
    "  \n",
    "  max_boxes_to_draw, nms_threshold, min_rpn_score_thresh, min_box_area = params\n",
    "  fig_size_h = min(max(5, int(len(category_names) / 2.5) ), 10)\n",
    "\n",
    "  #* /////////////////////////////////////////////////////////////////////\n",
    "  #* ///////////////// Obtain results and read image //// ////////////////\n",
    "  #///////////////////////////////////////////////////////////////////////\n",
    "  roi_boxes, roi_scores, detection_boxes, scores_unused, box_outputs, detection_masks, visual_features, image_info \\\n",
    "  = session.run(\n",
    "      ['RoiBoxes:0', 'RoiScores:0', '2ndStageBoxes:0', '2ndStageScoresUnused:0', 'BoxOutputs:0', 'MaskOutputs:0', 'VisualFeatOutputs:0', 'ImageInfo:0'], # fetches\n",
    "      feed_dict={'Placeholder:0': [image_path, ] }\n",
    "    )\n",
    "  \n",
    "  roi_boxes = np.squeeze(roi_boxes, axis=0)  # squeeze\n",
    "  roi_scores = np.squeeze(roi_scores, axis=0)\n",
    "  detection_boxes = np.squeeze(detection_boxes, axis=(0, 2))\n",
    "  scores_unused = np.squeeze(scores_unused, axis=0)\n",
    "  box_outputs = np.squeeze(box_outputs, axis=0)\n",
    "  detection_masks = np.squeeze(detection_masks, axis=0)\n",
    "  visual_features = np.squeeze(visual_features, axis=0)\n",
    "  image_info = np.squeeze(image_info, axis=0)  # obtain image info\n",
    "  \n",
    "  image_scale = np.tile(image_info[2:3, :], (1, 2))\n",
    "  image_height = int(image_info[0, 0])\n",
    "  image_width = int(image_info[0, 1])\n",
    "  rescaled_detection_boxes = detection_boxes / image_scale # rescale\n",
    "\n",
    "  ## Read image\n",
    "  image = np.asarray(Image.open(open(image_path, 'rb')).convert(\"RGB\"))\n",
    "  assert image_height == image.shape[0]\n",
    "  assert image_width == image.shape[1]\n",
    "\n",
    "\n",
    "  #* /////////////////////////////////////////////////////////////////////\n",
    "  #* ///////////////////////   Filter Boxed  /////////////////////////////\n",
    "  #///////////////////////////////////////////////////////////////////////\n",
    "  ## Apply non-maximum suppression to detected boxes with nms threshold.\n",
    "  nmsed_indices = nms(detection_boxes, roi_scores, thresh=nms_threshold)\n",
    "  ## Compute RPN box size.\n",
    "  box_sizes = (rescaled_detection_boxes[:, 2] - rescaled_detection_boxes[:, 0]) * (rescaled_detection_boxes[:, 3] - rescaled_detection_boxes[:, 1])\n",
    "  ## Filter out invalid rois (nms-okay rois)\n",
    "  valid_indices = np.where(\n",
    "      np.logical_and(\n",
    "                    np.isin(np.arange(len(roi_scores), dtype=np.int), nmsed_indices),\n",
    "                    np.logical_and(\n",
    "                                  np.logical_not(np.all(roi_boxes == 0., axis=-1)),\n",
    "                                  np.logical_and(\n",
    "                                                roi_scores >= min_rpn_score_thresh,\n",
    "                                                box_sizes > min_box_area \n",
    "                                                )\n",
    "                                  )    \n",
    "                    )\n",
    "  )[0]\n",
    "  print('number of valid indices', len(valid_indices))\n",
    "\n",
    "  detection_roi_scores = roi_scores[valid_indices][:max_boxes_to_draw, ...]\n",
    "  detection_boxes = detection_boxes[valid_indices][:max_boxes_to_draw, ...]\n",
    "  detection_masks = detection_masks[valid_indices][:max_boxes_to_draw, ...]\n",
    "  \n",
    "  detection_visual_feat = visual_features[valid_indices][:max_boxes_to_draw, ...]  ## Get Detection Visual Feat\n",
    "  rescaled_detection_boxes = rescaled_detection_boxes[valid_indices][:max_boxes_to_draw, ...]  ## Get Rescaled Detection Boxes \n",
    "\n",
    "\n",
    "  #* /////////////////////////////////////////////////////////////////////\n",
    "  #* // Compute text embeddings and detection scores and rank results ////\n",
    "  #! /////////////////////////////////////////////////////////////////////\n",
    "  ''' def build_text_embedding(categories):\n",
    "        if FLAGS.prompt_engineering:\n",
    "          templates = multiple_templates\n",
    "        else:\n",
    "          templates = single_template\n",
    "        with torch.no_grad():\n",
    "          all_text_embeddings = []\n",
    "          for category in tqdm(categories):\n",
    "            texts = [template.format(processed_name(category['name'], rm_dot=True), article=article(category['name'])) for template in templates]\n",
    "            if FLAGS.this_is:\n",
    "              texts = ['This is ' + text if text.startswith('a') or text.startswith('the') else text for text in texts]\n",
    "            texts = clip.tokenize(texts) # Tensor\n",
    "            text_embeddings = model.encode_text(texts)\n",
    "            text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "            text_embedding = text_embeddings.mean(dim=0)\n",
    "            text_embedding /= text_embedding.norm()\n",
    "            all_text_embeddings.append(text_embedding)\n",
    "          all_text_embeddings = torch.stack(all_text_embeddings, dim=1)\n",
    "        return all_text_embeddings.cpu().numpy().T '''\n",
    "  text_features = build_text_embedding(categories)\n",
    "  \n",
    "  raw_scores = detection_visual_feat.dot(text_features.T) ## From visual features\n",
    "  if FLAGS.use_softmax:\n",
    "    scores_all = softmax(FLAGS.temperature * raw_scores, axis=-1)\n",
    "  else:\n",
    "    scores_all = raw_scores\n",
    "\n",
    "  indices = np.argsort(-np.max(scores_all, axis=1))  # Results are ranked by scores\n",
    "  indices_fg = np.array([i for i in indices if np.argmax(scores_all[i]) != 0])\n",
    "\n",
    "\n",
    "  #* /////////////////////////////////////////////////////////////////////\n",
    "  #* ///////////  Plot detected boxes on the input image  ////////////////\n",
    "  #///////////////////////////////////////////////////////////////////////\n",
    "  ymin, xmin, ymax, xmax = np.split(rescaled_detection_boxes, 4, axis=-1)\n",
    "  processed_boxes = np.concatenate([xmin, ymin, xmax - xmin, ymax - ymin], axis=-1)\n",
    "  segmentations = paste_instance_masks(detection_masks, processed_boxes, image_height, image_width)\n",
    "\n",
    "  if len(indices_fg) == 0:\n",
    "    #//display_image(np.array(image), size=overall_fig_size)\n",
    "    print('ViLD does not detect anything belong to the given category')\n",
    "  else:\n",
    "    image_with_detections = visualize_boxes_and_labels_on_image_array(np.array(image),\n",
    "                                                                      rescaled_detection_boxes[indices_fg],\n",
    "                                                                      valid_indices[:max_boxes_to_draw][indices_fg],\n",
    "                                                                      detection_roi_scores[indices_fg],    \n",
    "                                                                      numbered_category_indices,\n",
    "                                                                      instance_masks=segmentations[indices_fg],\n",
    "                                                                      use_normalized_coordinates=False,\n",
    "                                                                      max_boxes_to_draw=max_boxes_to_draw, min_score_thresh=min_rpn_score_thresh,\n",
    "                                                                      skip_scores=False, skip_labels=True)\n",
    "    plt.figure(figsize=overall_fig_size)\n",
    "    plt.imshow(image_with_detections)\n",
    "    plt.axis('off')\n",
    "    plt.title('Detected objects and RPN scores')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "  #* /////////////////////////////////////////////////////////////////////\n",
    "  #* ////////////////////////////  Plot //////////////////////////////////\n",
    "  cnt = 0\n",
    "  raw_image = np.array(image)\n",
    "  n_boxes = rescaled_detection_boxes.shape[0]\n",
    "\n",
    "  for anno_idx in indices[0:int(n_boxes)]:\n",
    "    rpn_score = detection_roi_scores[anno_idx]\n",
    "    bbox = rescaled_detection_boxes[anno_idx]  ## Get BBox\n",
    "    scores = scores_all[anno_idx]\n",
    "    if np.argmax(scores) == 0:\n",
    "      continue\n",
    "    \n",
    "    y1, x1, y2, x2 = int(np.floor(bbox[0])), int(np.floor(bbox[1])), int(np.ceil(bbox[2])), int(np.ceil(bbox[3]))\n",
    "    img_w_mask = plot_mask(mask_color, alpha, raw_image, segmentations[anno_idx])\n",
    "    crop_w_mask = img_w_mask[y1:y2, x1:x2, :]\n",
    "\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(fig_size_w, fig_size_h), gridspec_kw={'width_ratios': [3, 1, 1, 2]}, constrained_layout=True)\n",
    "\n",
    "    # Draw bounding box.\n",
    "    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=line_thickness, edgecolor='r', facecolor='none')\n",
    "    axs[0].add_patch(rect)\n",
    "    axs[0].set_xticks([])\n",
    "    axs[0].set_yticks([])\n",
    "    axs[0].set_title(f'bbox: {y1, x1, y2, x2}   area: {(y2 - y1) * (x2 - x1)}   rpn score: {rpn_score:.4f}') ## Risk Priority Number (RPN)\n",
    "    axs[0].imshow(raw_image)\n",
    "\n",
    "    # Draw image in a cropped region.\n",
    "    crop = np.copy(raw_image[y1:y2, x1:x2, :])\n",
    "    axs[1].set_xticks([])\n",
    "    axs[1].set_yticks([])\n",
    "    axs[1].set_title(f'predicted: {category_names[np.argmax(scores)]}')\n",
    "    axs[1].imshow(crop)\n",
    "    # Draw segmentation inside a cropped region.\n",
    "    axs[2].set_xticks([])\n",
    "    axs[2].set_yticks([])\n",
    "    axs[2].set_title('mask')\n",
    "    axs[2].imshow(crop_w_mask)\n",
    "\n",
    "    # Draw category scores.\n",
    "    fontsize = max(min(fig_size_h / float(len(category_names)) * 45, 20), 8)\n",
    "    for cat_idx in range(len(category_names)):\n",
    "      axs[3].barh(cat_idx, scores[cat_idx], \n",
    "                  color='orange' if scores[cat_idx] == max(scores) else 'blue')\n",
    "    axs[3].invert_yaxis()\n",
    "    axs[3].set_axisbelow(True)\n",
    "    axs[3].set_xlim(0, 1)\n",
    "    plt.xlabel(\"confidence score\")\n",
    "    axs[3].set_yticks(range(len(category_names)))\n",
    "    axs[3].set_yticklabels(category_names, fontdict={'fontsize': fontsize})\n",
    "    \n",
    "    cnt += 1\n",
    "    #//fig.tight_layout()\n",
    "  ## FOR ENDING\n",
    "  print('Detection counts:', cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels embedding Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building text embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3/3 [00:10<00:00,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n",
      "[[-0.01189436 -0.00052207 -0.00349773 ... -0.01829072  0.02413167\n",
      "   0.04440202]\n",
      " [ 0.02237581  0.07252944 -0.04320214 ... -0.04653419  0.02202787\n",
      "  -0.03584645]\n",
      " [-0.01012344  0.03236039 -0.02046944 ... -0.0602644   0.00711658\n",
      "   0.01534752]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "labels += ['bracelet', 'umbrella'] \n",
    "labels += ['background']\n",
    "#//print(labels)\n",
    "labels = [{'name': item, 'id': idx+1,} for idx, item in enumerate(labels)]\n",
    "label_indices = {cat['id']: cat for cat in labels}\n",
    "#//print(labels)\n",
    "\n",
    "\n",
    "\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "multiple_templates = [\n",
    "    'There is {article} {} in the scene.', 'There is the {} in the scene.',\n",
    "    'a photo of {article} {} in the scene.', 'a photo of the {} in the scene.', 'a photo of one {} in the scene.',\n",
    "    # itap: I took a picture of\n",
    "    'itap of {article} {}.', 'itap of my {}.', 'itap of the {}.',\n",
    "    \n",
    "    'a photo of {article} {}.', 'a photo of the {}.', 'a photo of one {}.', 'a photo of many {}.',\n",
    "    'a good photo of {article} {}.', 'a good photo of the {}.',\n",
    "    'a bad photo of {article} {}.', 'a bad photo of the {}.',\n",
    "    'a photo of a nice {}.', 'a photo of the nice {}.',\n",
    "\n",
    "    'a photo of a small {}.', 'a photo of the small {}.',\n",
    "    'a photo of a large {}.', 'a photo of the large {}.',\n",
    "\n",
    "    'a bright photo of {article} {}.', 'a bright photo of the {}.',\n",
    "    'a dark photo of {article} {}.', 'a dark photo of the {}.',\n",
    "\n",
    "    'a photo of a hard to see {}.', 'a photo of the hard to see {}.',\n",
    "    'a low resolution photo of {article} {}.', 'a low resolution photo of the {}.',\n",
    "    'a cropped photo of {article} {}.', 'a cropped photo of the {}.',\n",
    "    'a close-up photo of {article} {}.', 'a close-up photo of the {}.',\n",
    "    'a jpeg corrupted photo of {article} {}.', 'a jpeg corrupted photo of the {}.',\n",
    "    'a blurry photo of {article} {}.', 'a blurry photo of the {}.',\n",
    "    'a pixelated photo of {article} {}.', 'a pixelated photo of the {}.',\n",
    "    'a black and white photo of the {}.', 'a black and white photo of {article} {}.',\n",
    "\n",
    "    'a painting of the {}.', 'a painting of a {}.',\n",
    "]\n",
    "\n",
    "clip.available_models()\n",
    "# print(clip.available_models()) # ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "\n",
    "\n",
    "\n",
    "def article(name): \n",
    "  return 'an' if name[0] in 'aeiou' else 'a'\n",
    "\n",
    "this_is = True\n",
    "def build_text_embedding(categories):\n",
    "  templates = multiple_templates\n",
    "  with torch.no_grad():\n",
    "    all_text_embeddings = []\n",
    "    print('Building text embeddings...')\n",
    "    for category in tqdm(categories):\n",
    "      texts = [template.format(category['name'], article=article(category['name']))\n",
    "        for template in templates]\n",
    "      if this_is:\n",
    "        texts = ['This is ' + text if text.startswith('a') or text.startswith('the') else text \n",
    "                 for text in texts]\n",
    "      texts = clip.tokenize(texts) #@ Returns a LongTensor containing tokenized sequences of given text input(s).\n",
    "      #TODO print(\"texts[tokenized sequences of given text inputs]: \", texts) ## This can be used as the input to the model.\n",
    "      \n",
    "      #@ Given a batch of text tokens, returns the text features encoded by the language portion of the CLIP model.  \n",
    "      text_embeddings = model.encode_text(texts) #embed with text encoder \n",
    "      text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "      text_embedding = text_embeddings.mean(dim=0)\n",
    "      text_embedding /= text_embedding.norm()\n",
    "      all_text_embeddings.append(text_embedding)\n",
    "    ## FOR ENDING\n",
    "    all_text_embeddings = torch.stack(all_text_embeddings, dim=1)\n",
    "  ## WITH ENDING \n",
    "  return all_text_embeddings.cpu().numpy().T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "text_features = build_text_embedding(labels)\n",
    "\n",
    "for label, feature in zip(labels, text_features):\n",
    "  label['embedding_feature'] = feature\n",
    "  #//print(label)\n",
    "  #//print(type(label))\n",
    "#//print(text_features)\n",
    "#//print(type(labels))\n",
    "\n",
    "with open('label_embedding.csv', 'w') as f:\n",
    "  for label in labels:\n",
    "      [f.write('{0},{1}\\n'.format(key, value)) for key, value in label.items()]\n",
    "  \n",
    "for feature in text_features:\n",
    "  #//print(feature)\n",
    "  pass\n",
    "\n",
    "print(type(text_features))\n",
    "np.save('label_embedding.npy', text_features)\n",
    "\n",
    "features = np.load('label_embedding.npy')\n",
    "import operator\n",
    "\n",
    "print(operator.eq(features, text_features))\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './examples/test.jpg'  #@param {type:\"string\"}\n",
    "#//display_image(image_path, size=display_input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'main' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/gsq/Desktop/zero_shot_object_detection/my_vild_demo.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gsq/Desktop/zero_shot_object_detection/my_vild_demo.ipynb#ch0000019?line=9'>10</a>\u001b[0m min_box_area \u001b[39m=\u001b[39m \u001b[39m220\u001b[39m \u001b[39m#@param {type:\"slider\", min:0, max:10000, step:1.0}\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gsq/Desktop/zero_shot_object_detection/my_vild_demo.ipynb#ch0000019?line=10'>11</a>\u001b[0m params \u001b[39m=\u001b[39m max_boxes_to_draw, nms_threshold, min_rpn_score_thresh, min_box_area\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gsq/Desktop/zero_shot_object_detection/my_vild_demo.ipynb#ch0000019?line=12'>13</a>\u001b[0m main(image_path, category_name_string, params)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'main' is not defined"
     ]
    }
   ],
   "source": [
    "''' e.g.\n",
    "category_name_string = ';'.join(['flipflop', 'street sign', 'bracelet', 'necklace', 'shorts', 'floral camisole', 'orange shirt', 'purple dress', 'yellow tee', 'electric box',\n",
    "                                 'green umbrella', 'pink striped umbrella', 'transparent umbrella', 'plain pink umbrella', 'blue patterned umbrella', 'koala', 'car', 'pole'])\n",
    "'''\n",
    "category_name_string = ';'.join(['bracelet', 'umbrella', 'bag', 'girl', 'dress', 'banner', 'slogan', 'mouth', 'streamer']) ## + background\n",
    "\n",
    "max_boxes_to_draw = 10 #@param {type:\"integer\"}\n",
    "nms_threshold = 0.6 #@param {type:\"slider\", min:0, max:0.9, step:0.05} -- Non Maximum Suppression\n",
    "min_rpn_score_thresh = 0.9  #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "min_box_area = 220 #@param {type:\"slider\", min:0, max:10000, step:1.0}\n",
    "params = max_boxes_to_draw, nms_threshold, min_rpn_score_thresh, min_box_area\n",
    "\n",
    "main(image_path, category_name_string, params)\n",
    "\n",
    "#@markdown Orange bar means the prediction with maximum score over text inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViLD is able to detect nothing when there is no matched category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fc/n5827zt91vb65jnqcsgpv5cr0000gn/T/ipykernel_18849/1381839267.py:52: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.isin(np.arange(len(roi_scores), dtype=np.int), nmsed_indices),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid indices 81\n",
      "Building text embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:10<00:00,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViLD does not detect anything belong to the given category\n",
      "Detection counts: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Same image, detect koalas\n",
    "category_name_string = ';'.join(['koala'])\n",
    "\n",
    "max_boxes_to_draw = 25 #@param {type:\"integer\"}\n",
    "nms_threshold = 0.6 #@param {type:\"slider\", min:0, max:0.9, step:0.05}\n",
    "min_rpn_score_thresh = 0.9  #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "min_box_area = 220 #@param {type:\"slider\", min:0, max:10000, step:1.0}\n",
    "params = max_boxes_to_draw, nms_threshold, min_rpn_score_thresh, min_box_area\n",
    "\n",
    "main(image_path, category_name_string, params)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86c8192e3b52dd5b61cf7d633d9cde44ca62fe25113112c353ce106148b537f5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('zeroShot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
